## Learning Report: 【漫士科普】90分钟深度！一口气看明白人工智能和神经网络#人工智能 #神经网络

### 1. Summary

This video provides a comprehensive, beginner-friendly introduction to Artificial Intelligence (AI) and Neural Networks, tracing their origins from the 1956 Dartmouth Conference to modern applications like GPT and AlphaGo. The presenter, a Tsinghua Yao Class AI Ph.D. student, demystifies AI by defining intelligence as the ability to respond purposefully to different situations based on collected information. The video explores two main historical AI paradigms: Symbolism, which uses logical rules, and Connectionism (Machine Learning), which emphasizes learning through data and adjustment. A significant portion is dedicated to explaining the Perceptron, its initial limitations with the XOR problem, and how Multilayer Perceptrons (MLPs) and subsequent neural network architectures like CNNs and Transformers overcame these challenges. The core training mechanism, Gradient Descent, and its computational backbone, Backpropagation, are explained in detail. Finally, the video addresses the critical concept of generalization and AI's current limitations, particularly regarding understanding causality versus correlation, ethical concerns, and the impact of AI on future employment, concluding that AI will likely transform, rather than eliminate, many jobs.

### 2. Key Takeaways

*   **Intelligence as Contextual Response:** AI fundamentally aims to build systems that gather information and provide targeted responses, mirroring biological intelligence.
*   **Evolution of AI Paradigms:** AI development progressed from Symbolism (rule-based logic) to Connectionism (data-driven learning via neural networks), with early neural networks like Perceptrons facing and overcoming significant limitations.
*   **Neural Networks as Universal Function Approximators:** Sufficiently deep and wide neural networks can theoretically model any input-output relationship, learning complex patterns and concepts hierarchically from raw data.
*   **Gradient Descent and Backpropagation are Core Training Mechanisms:** The powerful combination of Gradient Descent (iteratively adjusting parameters to minimize error) and Backpropagation (efficiently calculating these adjustments in complex networks) underpins nearly all modern AI training.
*   **Generalization is Key to AI Utility:** AI's ability to "generalize" or apply learned patterns to unseen data is what makes it powerful, but it struggles with distinguishing correlation from causation and is susceptible to adversarial attacks.
*   **AI's Impact on Work is Transformative, Not Purely Substitutive:** While AI will automate repetitive, data-rich tasks, human creativity, emotional depth, and real-world interactive capabilities remain critical and will likely lead to job transformation rather than complete replacement.

### 3. Timestamped Outline

*   **Introduction to AI and Neural Networks [00:00]**
    *   Addressing common questions about AI and its impact [00:00]
    *   Presenter's background and expertise [00:37]
    *   The 1956 Dartmouth Conference: Birth of AI [01:03]
    *   AI's impact: AlphaGo, GPT, Turing and Nobel Prizes [01:54]
*   **Defining Intelligence and AI [02:18]**
    *   What is intelligence? Observing intelligent behavior [02:18]
    *   Core definition: Collecting information and making targeted responses [03:07]
    *   Examples: Ciliates, facial recognition, AlphaGo, GPT [03:49]
    *   Intelligence as a "black box" function mapping input to smart output [04:36]
    *   Thomas Garrity's speech on functions [04:55]
    *   Turing Test: AI indistinguishable from human intelligence [05:40]
*   **Early AI Paradigms [06:05]**
    *   **Symbolism [06:17]**
        *   Simulating intelligence through formal mathematical logic [06:17]
        *   Example: Predicting rain based on conditions [06:29]
        *   Success: Expert systems in medicine and finance [07:29]
        *   Limitations: Lack of clear rules, inability to surpass human experts, static learning [07:44]
    *   **Machine Learning (Connectionism) [08:33]**
        *   Growth mindset: Black box learns through continuous adjustment [08:33]
        *   Analogy: Training a dog with rewards and punishments [08:52]
        *   Definition: Machines learning tasks by adjusting themselves [09:30]
        *   Power: No need for expert knowledge, only powerful black box and data [09:56]
        *   Example: Training a digit recognition system [10:14]
        *   Key questions: How does the black box learn? Rewards/punishments? Self-adjustment? [10:50]
*   **The Learning Black Box: Neural Networks [11:30]**
    *   **Connectionism [11:30]**
        *   Inspired by the human brain and its neurons [11:30]
    *   **Simplest Intelligence: Apple Recognition [12:00]**
        *   Concepts derived from attribute combinations [12:00]
        *   Representing features with 1s and 0s [12:40]
        *   Perceptron: Weighting features and summing them for a "score" [13:08]
        *   Threshold activation for decision making [14:02]
        *   Flexibility: Adjusting weights to identify different fruits (e.g., watermelon, hawthorn) [14:38]
        *   Pattern recognition [15:23]
    *   **The Perceptron [15:35]**
        *   Similarity to expert systems and logical reasoning [15:35]
        *   Numerical computation for broader potential [16:04]
        *   Biological neuron analogy: Dendrites, axons, electrical signals [16:32]
        *   Mathematical model by Pitts and McCulloch (1943) [17:16]
        *   First practical Perceptron by Rosenblatt (1957) [17:40]
        *   Recognizing men/women, arrows from pixels [17:53]
        *   The difficulty of computer vision for machines [18:22]
        *   Public excitement and over-optimism (New York Times, "electronic brain") [19:35]
        *   Human tendency to romanticize AI advancements [20:30]
    *   **Parameters and Models [20:47]**
        *   Models define function form (e.g., Perceptron's Wx - b) [20:47]
        *   Parameters (weights 'w' and bias 'b') are adjustable values [21:26]
        *   Connectionist belief: Powerful models + parameter adjustment = intelligence [21:38]
*   **AI Winter and the XOR Problem [22:04]**
    *   Early criticisms of Connectionism [22:04]
    *   Minsky's "Perceptrons" (1969) and the XOR problem [22:30]
        *   XOR: Output 1 if inputs differ, 0 if same [22:50]
        *   Perceptron's linear decision boundary [23:30]
        *   XOR points cannot be separated by a single line [24:14]
        *   Minsky's conclusion: Perceptrons are useless [24:43]
        *   AI Winter: Funding cuts, researchers leaving [25:06]
    *   **Resilience of Researchers [25:27]**
        *   Geoffrey Hinton's pride in sticking with neural networks [25:50]
*   **Multilayer Perceptrons (MLP) and Neural Networks [26:50]**
    *   Overcoming XOR with multiple neurons (layering Perceptrons) [26:50]
    *   Example: Two intermediate neurons for specific activations [27:30]
    *   Combining outputs to solve XOR [28:30]
    *   MLP: Multilayer Perceptron [28:56]
    *   Deep Neural Networks: Stacking layers of neurons [29:16]
    *   Universal Approximation Theorem: Sufficiently large networks can fit any function [29:40]
    *   Understanding the power: Combining simple concepts into complex ones [30:17]
        *   Example: Digit recognition (strokes to lines to numbers) [30:30]
        *   Automatic learning without human expert knowledge [31:20]
*   **Advanced Neural Network Architectures [31:40]**
    *   Beyond MLPs: Designing better structures [31:40]
    *   Convolutional Neural Networks (CNNs): Local connections, parameter sharing [32:10]
    *   Residual Networks (ResNet): Skip connections [32:50]
    *   DenseNet, Transformer (Attention) [33:10]
    *   Neural network structure design as a key field [33:40]
*   **Training Neural Networks: Gradient Descent [34:00]**
    *   How networks learn: Training with rewards/punishments [34:00]
    *   Gradient Descent: Core algorithm for almost all AI models [34:30]
    *   **Recap: Intelligence as Function Fitting [35:20]**
        *   Fitting data points with a function [35:20]
        *   Example: Fifth-degree polynomial fitting [36:20]
    *   **Loss Function [36:40]**
        *   Quantifying "goodness" of fit [36:40]
        *   Example: Least squares (sum of squared errors) [36:50]
        *   Measuring deviation between prediction and reality [37:46]
        *   Distinction: Fitting function vs. Loss function [38:00]
    *   **The Optimization Challenge [39:20]**
        *   "Game": Adjusting parameters (knobs) to minimize loss [39:20]
        *   Neural networks: Adjusting connection weights and biases [39:50]
        *   Problem: Too many parameters (e.g., GPT-3 with 175 billion) [40:15]
        *   Non-convex optimization: A notoriously difficult problem [40:50]
    *   **Solution: Gradient Descent and Backpropagation [41:00]**
        *   Seppo Linnainmaa (1976) and Backpropagation (Rumelhart, Hinton, Williams, 1986) [41:10]
        *   **Single Parameter Optimization [41:50]**
            *   Assuming other parameters are fixed [41:50]
            *   Observing loss function change with parameter adjustment [42:15]
            *   Analogy: Navigating a foggy mountain landscape [43:00]
            *   Using the derivative (slope of tangent) to find direction [43:40]
            *   "Take small steps in the direction of steepest descent" [44:30]
            *   Iterative process to reach the minimum [45:20]
        *   **Multiple Parameter Optimization [45:45]**
            *   Extending to two parameters (K1, K2): Loss surface [46:00]
            *   Partial derivatives: Changing one parameter while holding others constant [46:30]
            *   Gradient: Direction of fastest change (steepest slope) [47:20]
            *   Applying to N parameters (N-dimensional gradient) [48:00]
    *   **Backpropagation [48:50]**
        *   Calculating gradients for complex neural networks [48:50]
        *   Core idea: Functions as compositions of simple operations (building blocks) [49:30]
        *   Chain Rule: How derivatives propagate through composite functions [50:30]
            *   Example: f(g(x)) derivative is g'(x) * f'(g(x)) [51:40]
            *   Gear analogy: Transmission ratios [52:00]
        *   Working backward from loss function to each parameter [52:50]
        *   Summary: Backpropagation for gradient calculation, Gradient Descent for parameter adjustment [53:50]
*   **Generalization and AI's Limitations [54:50]**
    *   **Generalization: The ability to apply learned knowledge to unseen data [54:50]**
        *   Analogy: Curve fitting – inferring values between data points [55:40]
        *   Neural networks' powerful generalization: Learning abstract trends and patterns [56:50]
        *   Universal solution for complex, non-quantifiable patterns (e.g., Go strategy, language nuance, protein structure) [57:40]
        *   "Black box" learns subtle relationships and applies them [58:40]
    *   **AI is Not Omnipotent [59:00]**
        *   Human tendency to view AI as magic [59:00]
        *   **Correlation vs. Causation [59:50]**
            *   Chai dog vs. bread meme: Misinterpreting features based on correlation [59:50]
            *   Inability to grasp causal relationships [1:00:50]
            *   Ethical concerns: AI for fortune-telling, crime prediction (e.g., racial bias) [1:01:20]
        *   **Lack of Interpretability [1:02:00]**
            *   Difficulty understanding internal workings of complex networks [1:02:00]
            *   Adversarial examples: Subtle noise causing drastic misclassifications (e.g., panda to gibbon) [1:02:40]
            *   Ongoing research: Triggering "magical switches" in networks [1:03:50]
*   **AI and Future Employment [1:04:10]**
    *   Will AI cause job loss? [1:04:10]
    *   AI's impact on repetitive, pattern-fixed, data-rich jobs [1:04:40]
        *   Examples: Secretaries, illustrators, photographers, translators, finance, entry-level programmers [1:05:00]
    *   AI is not perfect ("artificial idiot") [1:05:40]
        *   Struggles with complex, out-of-training-data problems [1:05:50]
    *   **AI's Transformative Role [1:06:20]**
        *   Changing job nature rather than full replacement [1:06:20]
        *   Secretarial work: More efficient, human oversight for management and decision-making [1:06:40]
        *   Creative work (illustration, advertising): AI for initial drafts, human for emotional depth and creativity [1:07:20]
        *   Research: AI for writing, grammar, simple derivations; human for core ideas and innovation [1:07:50]
        *   Lack of real-world interaction: Self-driving cars, robotics still challenging [1:08:40]
    *   **Conclusion: Adapt and Collaborate [1:09:00]**
        *   AI will impact jobs but also create opportunities [1:09:00]
        *   Key: Adapting, upgrading skills, collaborating with AI [1:09:20]
        *   Human creativity, emotion, wisdom remain unique [1:09:40]
        *   Encouragement to learn AI [1:09:50]
*   **Closing Remarks [1:10:00]**

### 4. Key Concepts & Entities

*   **人工智能 (Artificial Intelligence - AI)**: A field of research focused on creating machines that can continuously learn and simulate human intelligence (0:30).
*   **神经网络 (Neural Networks)**: A type of AI model inspired by the human brain, simulating individual neurons and their complex connections to achieve intelligence (5:56).
*   **达特茅斯会议 (Dartmouth Conference)**: A pivotal meeting in 1956 considered the starting point for the field of artificial intelligence (1:00).
*   **麦卡锡 (McCarthy)**: Dartmouth mathematics professor and a key participant in the Dartmouth Conference (1:11).
*   **闵斯基 (Minsky)**: Harvard University researcher in mathematics and neuroscience, also a participant in the Dartmouth Conference (1:13).
*   **罗切斯特 (Rochester)**: IBM executive who participated in the Dartmouth Conference (1:14).
*   **香农 (Shannon)**: Inventor of information theory, present at the Dartmouth Conference (1:15).
*   **图灵测试 (Turing Test)**: A test proposed by Alan Turing to determine if a machine can exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human (3:37).
*   **符号主义 (Symbolism)**: An AI paradigm that proposes intelligence can be simulated using formal mathematical reasoning and symbolic logic rules (4:00).
*   **专家系统 (Expert Systems)**: A successful application of symbolism that stored human expert knowledge as logical rules to provide predictions, particularly in medical diagnosis and financial consulting (4:49).
*   **机器学习 (Machine Learning)**: An AI paradigm where machines learn to solve tasks by adjusting themselves through rewards and punishments, without requiring explicit expert knowledge of internal structures (5:30).
*   **联结主义 (Connectionism)**: An AI paradigm that believes intelligence can be achieved by biologically mimicking the complex functions of individual neurons and their connections (6:08).
*   **感知机 (Perceptron)**: The earliest proposed pattern recognition algorithm and a foundational model in connectionism, capable of identifying patterns by weighting input features and activating based on a threshold (8:50).
*   **异或 (XOR)**: A basic logical operation that a single perceptron cannot perform, highlighted by Minsky as a fundamental limitation (15:58).
*   **多层感知机 (Multilayer Perceptron - MLP)**: An advancement over the single perceptron, consisting of multiple layers of neurons, capable of solving non-linear problems like XOR (18:48).
*   **卷积神经网络 (Convolutional Neural Network - CNN)**: A type of neural network that mimics animal visual systems by connecting neurons locally and sharing parameters, reducing complexity and improving performance (22:15).
*   **残差网络 (ResNet)**: A type of convolutional neural network that adds skip connections to overcome training difficulties in very deep networks (22:42).
*   **Transformer**: The fundamental framework underlying modern large language models like GPT, based on the attention mechanism (22:56).
*   **梯度下降 (Gradient Descent)**: A fundamental optimization algorithm used to train neural networks by iteratively adjusting parameters in the direction that minimizes the loss function (25:22).
*   **损失函数 (Loss Function)**: A quantitative measure that evaluates how well a model's predictions align with actual data, with a smaller value indicating a better fit (28:03).
*   **反向传播 (Backpropagation)**: A specialized algorithm used to efficiently calculate the gradients for complex neural networks, enabling the application of gradient descent (33:04).
*   **泛化 (Generalization)**: The ability of a machine learning model to apply learned patterns and trends from training data to new, unseen data, enabling it to "learn by analogy" (39:48).

### 5. Memorable Quotes

*   "人工智能，说白了就是人工搭建起一套智能。要实现这个目标，首先就要回答一个非常根本的问题：什么是智能？" (1:58) - *Translation: "Artificial intelligence, in simple terms, is the artificial construction of intelligence. To achieve this goal, we must first answer a very fundamental question: what is intelligence?"*
*   "智能的本质就是一个不会乱来的黑箱，或者用数学一点的说法，智能就是找到情景信息的输入和我们想要的聪明的行为输出之间的函数对应关系。" (3:21) - *Translation: "The essence of intelligence is a 'black box' that doesn't act randomly, or, in more mathematical terms, intelligence is finding the functional correspondence between situational information input and our desired intelligent behavior output."*
*   "我想我最骄傲的是我当年坚持了神经网络，尽管当时人们都说这是垃圾，而且说了整整40年。" (18:02) - *Translation: "I guess I'm proud of the fact that I stuck with neural networks, even when people said they were rubbish, which was for about the first 40 years."*
*   "很多时候做的比人都好。这种公式一样的解决方案非常通用，因而席卷了各个领域，引发了这些年的人工智能革命。" (42:33) - *Translation: "Many times, it performs better than humans. This formula-like solution is very versatile, sweeping across various fields and triggering the artificial intelligence revolution of recent years."*
*   "人类的创造力、情感和智慧，依然是AI无法模拟和完全超越的。" (48:06) - *Translation: "Human creativity, emotion, and wisdom are still things that AI cannot simulate or completely surpass."*

### 6. AI-Generated Q&A

**Q1: What are the fundamental differences between Symbolism and Connectionism as approaches to AI?**

**A1:** Symbolism aims to simulate intelligence by formalizing human knowledge and logic into symbolic rules and performing logical deductions (4:00). Its strengths lie in clear, rule-based reasoning but it struggles with unclear rules and cannot surpass expert-level knowledge (4:59). Connectionism, on the other hand, mimics the biological structure of the brain by connecting artificial neurons (6:08). It focuses on allowing a "black box" to learn and adapt through data and feedback, without requiring explicit human-designed rules, enabling it to discover abstract patterns and potentially exceed human performance in specific tasks (5:30, 21:05).

**Q2: How does a neural network learn to "understand" patterns and generalize to new data?**

**A2:** A neural network learns by adjusting its internal parameters (like connection weights and biases) through a process called training (23:40). This involves feeding it data, comparing its predictions to the actual outcomes, and using a "loss function" to quantify the error (28:03). The network then uses algorithms like "gradient descent" and "backpropagation" to iteratively tweak its parameters in a direction that minimizes this error (33:04). Through this continuous adjustment and exposure to vast amounts of data, the network discovers underlying trends and subtle correlations within the data, allowing it to "generalize" or apply this learned understanding to make reasonable predictions on unseen, new data (39:48).

**Q3: What are some critical limitations of current AI, particularly deep learning, and how might it impact human jobs?**

**A3:** Current deep learning AI faces several limitations. It struggles with distinguishing correlation from causation, often making errors when presented with data outside its training distribution, as illustrated by the "Chihuahua vs. Muffin" example (43:08). Its complex "black box" nature makes it difficult to understand *how* it arrives at predictions, leading to issues like "adversarial examples" where tiny, specially designed noise can drastically alter its perception (45:10). Regarding jobs, AI is likely to impact roles that are data-rich, pattern-fixed, and highly repetitive, such as administrative tasks, basic illustration, translation, and some entry-level programming (46:57). However, the speaker believes AI will more likely change the nature of work rather than fully replace it, making humans more efficient in supervisory and decision-making roles. Human creativity, emotional depth, and ability to interact in the real world remain areas where AI is currently limited (47:30, 48:06).

## 8. Formatted Transcript

[0:00:00] Artificial intelligence, machine learning, neural networks – these high-sounding terms, do you still only have a partial understanding? GPT large language models, generative AI – these cutting-edge technologies frequently appearing in business reports and even daily life, are you curious about their development, origins, and basic principles? In an era where everything can be AI, are you worried about being scammed or passively losing your job? This video took half a year to produce, specifically for zero-basis viewers, and will thoroughly explain the ins and outs of artificial intelligence. I am Manshi, an AI doctoral student graduated from Tsinghua's Yao Class. From undergraduate to graduate school, I received full marks in all AI courses, ranking first in my cohort. I am currently researching deep learning theory, having just returned from Princeton. Video production is not easy, so I hope for your likes and support. If you can't finish it, you can save it or follow me to watch it later.

[0:01:13] In 1956, a group of scientists gathered at Dartmouth for a conference. This conference lasted for over a month, with only about ten participants, but each one was a giant in their field. They included Dartmouth mathematics professor McCarthy, Harvard University mathematics and neuroscience researcher Minsky, IBM executive Rochester, and the inventor of information theory, Shannon, among others. Yet, this small conference profoundly changed the shape of the world decades later. AlphaGo, which defeated Lee Sedol 8 years ago, and GPT, which can now help us solve various daily problems, are essentially all rooted in the research initiated by this conference. How can we create a machine that can continuously learn and simulate human intelligence? This field, after receiving the Turing Award in 2018, also claimed the Nobel Prize in Physics and Chemistry this year. Yes, this field is called artificial intelligence, and this conference is considered the starting point for its creation. The Dartmouth Conference.

[0:02:07] Artificial intelligence, put simply, is the artificial construction of an intelligent system. To achieve this goal, we must first answer a very fundamental question: What is intelligence? Let's think about when in life we would consider an object to be intelligent. For example, we talk to a dog, tell it to sit, and it sits. Tell it to bark, and it barks twice. You scold it a couple of times, and it stares at you with a resentful expression. At this point, we think, "Ah, this little dog is very smart, it has spirit." Essentially, this means the dog is an intelligent creature, unlike a stone. No matter how much you call a stone, it remains unresponsive, thus it is inanimate. Therefore, although there are thousands of definitions of intelligence, they all revolve around a core idea: intelligence is essentially about collecting information and making targeted responses to different situations.

[0:02:48] In middle school, there was a very interesting experiment: observing the stress response of paramecia. If you drop some meat juice at one end, the paramecia will move towards it. If you add some salt, they will scatter and flee. It is precisely this movement that avoids harm and seeks benefit in response to environmental changes that leads us to believe these tiny particles in the water are intelligent organisms, not lifeless dust. In fact, if you see some paramecia that no longer exhibit this avoidance behavior, it indicates a simple truth: they are dead. Intelligence disappears with life. Since intelligence, in essence, is about acting according to circumstances, then artificial intelligence is also about building a system that provides targeted outputs and responses based on different environmental information. This output can be actions, language, or even judgments and predictions. For example, facial recognition can identify different people by providing their identity information based on different faces. AlphaGo can provide the best move for various complex game situations to strive for victory. GPT can converse with you and solve problems based on the context, different questions, and task requirements. Therefore, they are all intelligent.

[0:03:43] Imagine if facial recognition mistook everyone for Zhang San, AlphaGo made random moves on the chessboard, or GPT's answers were nonsensical. Then, the full name of this AI should be "artificial idiot." In summary, the essence of intelligence is a black box that doesn't act randomly. Or, to put it more mathematically, intelligence is finding the functional correspondence between input of contextual information and the desired intelligent behavior output.

[0:04:03] Here, I want to play a passionate speech from an American elder, Thomas Garrity. I believe you will gain a deeper understanding of this function.

[0:04:07] "Functions describe the world! Everything is described by functions. The sound of my voice on your eardrum, function! The light that's kind of hitting your eyeballs right now, function! The entries you put in your random matrices, function! It's all function! Different classes and mathematics, different areas and mathematics study, different kinds of functions. High school math studies, second degree one variable polynomials. Calculus studies, smooth one variable functions, and it goes on and on. Functions describe the world!"

[0:04:36] You can also better understand the Turing Test. The Turing Test states that if a human cannot distinguish between chatting with a human and an AI, then that AI has achieved human intelligence. It essentially means that AI, in essence, is the black box functional relationship from input to output that it defines. Its responses to you are indistinguishable from a human's, and thus it possesses human-like intelligence.

[0:04:54] The question arises: how do we create such an intelligent black box? Scientists have proposed many approaches to this problem. For example, one group of people drew inspiration from formal reasoning systems in mathematics, arguing that intelligence can be simulated through symbolic logical reasoning. This is called symbolism. For instance, let 'a' represent a cloudy day, 'b' represent humidity greater than 70%, and 't' represent that it will rain. Then, knowledge is a logical rule: if 'a' and 'b' are true, then 't' is true. Intelligence, in this context, means seeing a cloudy day and high humidity, understanding that 'a' and 'b' are true, and then using this rule to deduce that 't' is true, thereby predicting that it might rain. These human inferences and thoughts can be calculated like arithmetic operations from these symbols and rules. Therefore, symbolism believes that intelligence is a computational system that processes symbols and rules like this. It stores human knowledge and logic in rules like "if a, then b," and then through continuous symbolic inference, it can achieve intelligence close to that of humans.

[0:05:43] The most successful paradigm of this approach was the expert system. It consulted human medical experts about what symptoms might correspond to what diseases, then recorded all these rules. By using these rules, it could provide predictions based on existing information, creating an intelligent black box. This system once achieved great success in fields like disease diagnosis and financial consulting. However, as time went on, it gradually revealed fatal flaws. Firstly, in many cases, the real world does not have such clear-cut rules. Even when asking human experts whether a certain symptom indicates a particular disease, or whether a stock trend indicates buying or selling, their answers are often inconsistent. Your system can only choose one. Who should it listen to? More critically, this system merely replicates human experience, so its upper limit of capability is the expert's level; it cannot surpass humans. Furthermore, from the moment you design this system, its level remains static and unchanging. It is difficult for it to continuously improve its performance with experience and time, like a human.

[0:06:41] Consequently, from the 1970s onwards, another school of artificial intelligence began to flourish. It did not aim to have a perfect black box from the outset but allowed this black box to change continuously. Through constant guidance and learning, it could perform better and better on a particular task. Yes, this is also a growth mindset. This approach is like training a dog. You give the command "sit." If it sits, you reward it with dog food or a pat on the head. If it doesn't understand or just stares at you, you might scold it. Over time, the dog, this black box, will increasingly understand the relationship between your command "sit" and the action of sitting. It's like a conditioned reflex. After that, whenever it hears the command "sit," it will perform the action of sitting. Thus, a smart dog that can follow commands is trained.

[0:07:25] The name of this school is also vivid: machine learning. As the name suggests, the object of learning is shifted from dogs to machines. By rewarding or punishing machines, they are allowed to adjust autonomously, learn continuously, and thus acquire the intelligence to solve a specific task. This task could be recognizing digits and faces in images, playing Go, or conversing with people, and so on. The power of machine learning lies in its lack of need for any expert knowledge to manually construct the internal structure of the black box. It only requires two things: a powerful black box with learning capability, and sufficient data. For example, suppose you want an intelligent black box that can recognize digits. You only need to prepare a machine with learning capabilities, then collect many images of digits, and manually label what digit each image contains. Next, you just need to present these images to the machine one by one, like training a dog, and ask it to predict what digit is in the image. If it predicts correctly, you reward it; if it's wrong, you punish it. This allows the machine to continuously self-adjust. As it sees more and more images, it will magically be able to correctly identify the digit in the image.

[0:08:31] I'm sure you must have a question in your mind: where does such a magical black box come from that can become smarter and smarter with learning, like a dog or a human? And how do we reward and punish a machine? Is it by giving it more electricity? Moreover, the conditional reflexes and learning processes of dogs are so complex; how does a machine built by humans self-adjust and learn? Ah, these are excellent questions. These three questions actually correspond to the model, structure, loss function, and training process of machine learning, respectively. The following video will provide detailed answers. Let's first address the first question: how do we build a black box machine with learning capabilities? Is there a universal, super-powerful black box that can represent and learn any correspondence?

[0:09:11] Here, it is worth mentioning another school of artificial intelligence: connectionism. They believe that nature has already provided the standard answer for achieving intelligence: the intricate human brain. By simulating the complex functions of individual neurons and their intricate connections in a bionic way, we can achieve incredible intelligence, just like running a precise clock. This school of thought is called connectionism.

[0:09:36] To understand connectionism, let's first consider some simplest forms of intelligence. For example, here is an apple. Why do you think it's an apple? You might say because its diameter is about 10 centimeters, its skin is red, it's spherical, and it smells sweet. Indeed, our understanding of many concepts, like apples, relies on combinations of other conceptual attributes. For each different conceptual attribute, different fruits have their own characteristics. For instance, in terms of size, watermelons are large, while other fruits are small. Therefore, on the characteristic of large size, watermelons fit, while other fruits do not. In the computer world, we use 1 to represent "fits" and 0 to represent "does not fit." This way, we can list a relationship table between various fruits and different attributes. When the characteristics of each fruit match the attributes of an apple, we tend to judge it as an apple. Thus, we possess a simple intelligence for identifying fruits.

[0:10:22] Therefore, there is a simplest way to build a black box. For example, if we want to identify an apple, we take all the features of a fruit, such as size, color, smell, etc., as input, and then consider whether each of its features is like an apple. Specifically, it's multiplied by a coefficient. For instance, if the size is not large, the color is red, and the taste is sweet, these indicate that the fruit fits the characteristics of an apple, thus contributing positively to the judgment of it being an apple. We multiply them by a positive number. On the other hand, if the size is large and it tastes sour, these features indicate it's less likely to be an apple, so we multiply them by a negative coefficient. Finally, we just need to multiply each of these features by their corresponding coefficients and add them together to get an "apple score." The higher this score, the more it resembles an apple; the lower, the less so. Therefore, we can set a score threshold 'b'. If the final score is higher than 'b', this machine activates; otherwise, it does not. At this point, this entire machine becomes an apple identification machine. Only when you place an apple in front of it will it activate and light up the right-side light bulb. As long as the fruit you place in front does not meet the characteristics of an apple, it will not activate. So, according to our definition, the entire black box now possesses the intelligence to identify apples.

[0:11:23] The remarkable aspect of this black box machine is that you can not only use it to identify apples but also other fruits. By adjusting these connection coefficients, you can represent different concepts. For example, you can make it activate when a fruit is large, green, and sweet. In this case, the machine's purpose is to identify a watermelon. Or, if it activates when it's small, red, and sour, it can specifically screen for hawthorns. The coefficients connecting each feature to the output are like the knobs of a machine. By setting the values of these knobs according to your needs, you can make the entire machine specifically activate for a certain fruit. This process of identifying different fruit features from input data and understanding concepts is called pattern recognition. And this, in 1956, was one of the earliest pattern recognition algorithms proposed by humans: the perceptron.

[0:12:17] Some might say, "This process of combining various conditions to make a judgment seems no different from the expert systems we discussed earlier." Ah, you are quite right. This is a little-known fact. Many people assume that connectionism, represented by neural networks, was always at odds with symbolism. But in reality, the perceptron, the earliest type of neural network, was largely inspired by, or even derived from, logical reasoning. Its approach was also to combine different feature conditions for inference. Here, each neuron is like a proposition letter in symbolic logic, as we just discussed. However, it uses numerical computation to simulate logic. Numerical computation itself is not limited to finite and explicit symbolic reasoning, thus having greater potential in broader fields such as control, environmental perception, and image recognition. We will see this later. Coincidentally, this design also aligns with neuroscience. In middle school, we learned that neurons transmit information through electrical signals. Dendrites receive electrical signals from different neurons, while axons transmit their own signals to other neurons. The perceptron is also akin to a neuron. You see, the incoming numbers represent various signals it receives. They can either activate or inhibit the neuron's activity. This influence is reflected in the positive or negative sign and magnitude of the parameters 'w' on each connection. The neuron sums up all received influences, and if the accumulated stimulation is large enough, it activates and transmits further. All of this is in line with the activity of biological neurons. In fact, the proposal of this mathematical model predates artificial intelligence. As early as 1943, during World War II, Pitts and McCulloch jointly published "A Logical Calculus of the Ideas Immanent in Nervous Activity" in the Bulletin of Biophysics, proposing this mathematical model of neurons. Notice the title: "A Logical Calculus of the Ideas Immanent in Nervous Activity." This further indicates that connectionism and symbolism are actually co-originating.

[0:13:44] In 1957, Frank Rosenblatt at Cornell University built the first practically applicable perceptron. This perceptron took image pixels as input, adjusted parameters through algorithms, and learned continuously, eventually being able to determine if an image was of a man or a woman, or a left arrow or a right arrow. It caused a sensation at the time. Although it might seem insignificant today, let me explain it from a different perspective, and you will realize how remarkable it was. Computers and humans are very different. Problems that are difficult for humans, such as multiplying two ten-digit numbers, are simple for computers. But things that are simple for humans are very difficult for computers, such as understanding image content. Because, to a computer, an image is essentially a pile of pixel values. Therefore, recognizing content in an image is mathematically equivalent to giving you a matrix composed of a large number of digits and asking, "What is actually drawn here?" or "Is this a man or a woman?" Human faces vary greatly; subtle changes in various facial features, lighting, and angles will result in different images, causing significant changes in the specific pixel values. We need to use pure computation to figure out if the person in a real photo is male or female. Now, do you still think this entire mathematical problem, that is, understanding content from an image, is easy? Aren't human vision and brains incredible? This field is called computer vision.

[0:14:55] Consequently, after inventing the perceptron, 30-year-old Rosenblatt was full of ambition and eagerly held a press conference to talk about the bright future of his research, attracting great attention from many media outlets. For example, a famous New York Times reporter praised the perceptron's advanced nature, reporting that it was the prototype of an electronic computer that could walk, see, write, self-replicate, and possess self-awareness. He called it an "electronic brain." The term "computer" also originated around this time. The article was very optimistic, estimating that with another $100,000, the aforementioned concepts could be realized within a year. At that time, the perceptron would be able to recognize people and call them by name, and even translate and record people's speeches in real-time into another language. However, as those of us who have experienced reality know, this has only truly been achieved in recent years. So, whether it was sincerity or tricking investors, humans are easily romantic about things they don't understand and tend to be overly optimistic about the future. Historically, whenever artificial intelligence made even a slight progress, humans would begin to imagine boundless capabilities for it, envisioning scenarios of battles with AI. It has always been this way.

[0:15:59] Returning to the initial question: how exactly do we build an intelligent black box with learning and evolutionary capabilities? The perceptron is a simple example. Professionally speaking, these things are called models. The model itself determines the functional form of its output. For example, here, it first uses weighted combinations of all inputs 'wi' and then subtracts the threshold 'b' before activation, which is the output of its entire model. This is a functional form. However, at the same time, it does not fully determine the entire function. There are still a series of values that you need to adjust and set. For instance, each 'wi' and 'b' here are called parameters. We only need to make the model powerful enough during design, and essentially any function you want it to achieve can be realized by setting these parameters. Then, we only need to let this model continuously adjust its parameters, constantly changing towards a more useful output pattern that meets our needs, to ultimately achieve powerful intelligence. This is the belief of connectionism.

[0:16:49] This narrative, when first proposed, was ambitious and achieved remarkable accomplishments like the perceptron. However, connectionism once fell into a winter, even being denounced as a fraud by the whole world. From the very beginning, many scholars opposed connectionism, thinking it was merely a mechanical simulation of biological structures, and that the neuron models were too simplistic. They also believed that connectionism expected magic to happen through random connections. In 1969, Marvin Minsky wrote a book called "Perceptrons." Just as a thumbs-up gesture from someone might not be praise but rather an intention to hit you with a cannonball, his book was not intended to promote perceptrons but to deliver a death blow to them, hammering the final nail into their coffin. Minsky pointed out a fact in his book: a very basic operation in logic circuits, called XOR (exclusive OR), is impossible for the perceptron to perform. Simply put, when the two input features are the same, the output is 0; when they are different, the output is 1. This is a very simple correspondence.

[0:17:36] Why is this the case? If we carefully examine the functional form of the perceptron, we find that it essentially calculates the sign of W1x + W2y - b. If we plot all such inputs (x, y) on a 2D plane, all inputs that allow the perceptron to activate satisfy W1x + W2y - b > 0. Those familiar with middle school math will recognize this as a linear equation. Therefore, the boundary separating the inputs that satisfy the condition and those that do not will always be a straight line. However, for the XOR problem, you'll find that the two points that need to activate and the two points that do not activate are distributed in such a way that you can never separate these two types of points with a single straight line. Therefore, no perceptron can directly achieve this simple XOR operation. In his book, Minsky bluntly stated that Rosenblatt's paper had no scientific value. Minsky himself received the Turing Award in the same year, and this book consequently relegated the entire field of connectionism to obscurity. For the next two to three decades, the name "neural network" became almost synonymous with fraud, considered a useless toy that couldn't even perform the XOR operation. With the leading figures who had received the Turing Award decrying it, neural networks entered an extreme winter. Everyone considered them garbage and scams. Funding for research greatly decreased, researchers switched fields, and AI research entered a decades-long cold spell.

[0:18:57] You see, humans are so easily pessimistic. A small counterexample leads them to abandon a research direction full of potential. However, during this time, a group of researchers persisted. They eventually saw the dawn after the clouds and became the founders of later deep learning, receiving the Turing Award. We can listen to Geoffrey Hinton, a double laureate of the Turing Award and Nobel Prize, in an interview at UCD University:

[0:19:15] "Looking back at your entire academic career, what aspects of it are you most proud of? I'm not necessarily just thinking about your discoveries, but maybe other aspects of your career as well, the people who worked with you, the teams that you've built. I guess I'm proud of the fact that I stuck with neural networks even when people said they were rubbish, which was for about the first 40 years. But the intellectual achievement I'm most proud of is Boltzmann machines, which were an alternative to back propagation."

[0:19:39] How did they save neural networks? Let's go back to the XOR example. They thought, "If one neuron isn't enough, can multiple neurons work?" For instance, let's take the outputs of these perceptrons one by one and then nest another layer of perceptrons as the input for the next perceptron. It's like a set of nested dolls, one layer after another, creating a new perceptron. This way, we can have the two neurons in the middle layer be activated specifically by (0,1) and (1,0). For example, the first neuron has combination coefficients of 1 and -1. At this point, only the input (1,0) will provide it with the strongest stimulation, with a magnitude of 1; others will not exceed 0. So, if we set a threshold of 1/2, it will activate only for the input (1,0). Similarly, we can set up another neuron to activate only for (0,1) with coefficients -1 and 1. In this way, the two middle neurons can respectively focus on the two positions we want to activate. Then, we simply add the outputs of these two neurons together. If the sum is greater than 0, it gives the final activation. Thus, the entire model will activate precisely at (1,0) and (0,1), but not at (0,0) and (1,1), thereby achieving the XOR function. And this is the later famous MLP, Multilayer Perceptron. Here, the middle layer can of course have more than two neurons, and the number of layers can also be more than two. When these neurons are stacked layer upon layer, they form the famous neural network.

[0:20:50] Each connection here signifies the strength of connection between two neurons, which is an adjustable parameter coefficient. Computer scientists have proven that as long as the depth and width of this neural network are sufficiently large, it can theoretically approximate any function and express the input-output correspondence required for any intelligent function. In other words, as long as you have a super large neural network, any desired intelligent black box function can certainly be achieved by setting a set of parameters.

[0:21:18] How can we understand the powerful capability of neural networks? Remember what we said earlier? By combining different attribute concepts and then activating, we obtained the intelligence of a perceptron that could identify apples. If we continue to nest perceptrons, we can progressively combine simple, basic concepts into more complex ones. For example, in a neural network for digit recognition, the neurons at the very front are responsible for identifying very basic strokes and edges. Neurons in later layers are responsible for combining these basic features to identify more complex concepts, such as circles, horizontal lines, vertical lines, broken lines, and so on. Then, deeper neurons can combine these lines and shapes to recognize complex digits, such as a '9' which is a circle plus a tail at the bottom right. As the layers deepen, the neural network gradually deduces complex overall forms from simple features, ultimately accurately recognizing a complex concept. And this entire process requires no human expert knowledge; it is completed automatically. This is precisely the power of neural networks.

[0:22:16] As time progresses, neural network technology continues to advance. The multilayer perceptron we saw earlier is just the most classic and basic type. How to design better and more powerful model structures has always been an important topic in deep learning. For example, in the visual neural systems of animals in the real world, neurons do not need to be densely connected to all neurons in the previous layer; they only need to connect to a few local neurons. Furthermore, the parameters and structure of connections for each neuron to the previous layer are similar. Therefore, when designing neural networks, we can also draw inspiration from this to reduce parameters and computational load, thereby improving neural network performance. This is the famous Convolutional Neural Network (CNN). Later, researchers discovered that training too many convolutional layers became difficult, so they added a type of skip connection, which is the Residual Network (ResNet). Or, you can have skip connections between any two layers, which is DenseNet. And today, the basic framework of GPT, the Transformer, which is attention, are essentially some fundamental network framework structures, with a large number of parameters to be determined. A good structure allows the black box to learn faster and require less data. This is what deep learning was once an extremely important field: neural network structure design.

[0:23:25] You will surely ask, "Neural networks are so powerful; they can independently discover structures hidden in data and understand concepts. How exactly do they achieve this?" The answer is by training with data, using rewards and punishments to guide the neural network to form intelligence. But how should we reward and punish a neural network? In fact, from GPT to AlphaFold, then to Midjourney and various reinforcement learning models, almost all complex and advanced AI models use the same algorithm to train networks and find the best parameters. This algorithm is called gradient descent.

[0:23:55] Special note: the mathematical knowledge in this part is extensive and highly technical, but because it is so important, we must discuss it. Therefore, if you really don't understand it, it's okay. Specifically, I will follow the video and ideas of YouTuber Artem Kirsanov to explain it. Before explaining gradient descent, let's briefly review the previous content. We first mentioned that the essence of intelligence is a black box that can find the correspondence between input and output from data. In other words, from the perspective of data-driven machine learning and statistical learning, intelligence is essentially giving you a set of points and finding a function that describes the relationship between them. Here, 'x' and 'y' can be any two quantities you care about. As long as we learn a function that can characterize the trend of these points, we can obtain a reasonable output for any given input. In other words, intelligence is achieved.

[0:24:40] How do we find the underlying patterns outlined by these data points? Based on the previous content, you might think of neural networks. This is certainly one way. However, here, to understand gradient descent, let's use a simpler method to find this function. For example, let's linearly combine the constants, x, x squared, x cubed, x to the fourth power, and x to the fifth power – these simple monomial modules. In other words, we want to find a quintic polynomial to characterize the pattern of these data variations. We need to find the best combination of these six parameters: K0 to K5. What constitutes a good combination of parameters? We need a quantitative way to measure how well a set of coefficients, corresponding to a polynomial, fits the data. This is the loss function. Everyone has encountered loss functions before. In middle school, we learned about the method of least squares, which uses a simple linear function y = kx + b to build a black box. For each data point, there will be a deviation between the linear function's prediction and the actual result. We sum the squares of these deviations to get the loss function for this line. In complex nonlinear cases, the loss function is similar. We also sum the squared errors between the function's predicted values and the actual data points to get the loss function for this function. You can see that as the function's prediction more closely fits the trend of these data points, the total loss function will be smaller. Conversely, the loss function will be larger. Generally speaking, the loss function measures the degree of deviation between a model's prediction and the actual result. Just remember, mastering the pattern means the loss function is small.

[0:25:54] Please note that two types of functions appear here; don't confuse them. The first is the curve we use to fit the data points, which we call the fitting function, that quintic polynomial. Its input is 'x', and its output is 'y'. We need to determine these six parameters. There are infinitely many possible functions from input to output. We want to find the best one. And what is "best"? For this, we propose the loss function. It measures how good a fitting function is; it's a scoring machine. Its input is the six coefficients of the polynomial. After receiving these coefficients, it first constructs the fitting curve function and then compares and calculates the deviations at all data points, summing their squares to get the final output of the loss function. We only need to find the input parameter combination K0 to K5 that minimizes this loss function to find an excellent fitting function. With this fitting function, we can then take this machine, input any 'x' we care about, and get a 'y' that conforms to the data's pattern. You can think of it as playing a game. Each parameter 'k' is a knob. By setting these coefficients, they produce different polynomial curves. Your goal is to adjust these knobs to make the fitting function's curve closely align with the data points.

[0:26:56] In fact, what neural networks do is essentially the same. Just replace 'k' here with the connection coefficients and threshold 'b' between neurons, and training a neural network is also a game of adjusting parameter knobs to reduce the loss function. The difficulty of this game lies in the sheer number of knobs. Look, this quintic polynomial has six parameter knobs, which is already overwhelming. The number of parameters in a neural network is even more astonishing. For example, GPT-3 has 175 billion parameters. In other words, you need to adjust over 100 billion knobs simultaneously and ensure that the combined settings of these knobs result in good performance, allowing it to converse with you and solve problems. Doesn't that sound incredible? This is almost an impossible task. Mathematically, this problem is called non-convex optimization, and its difficulty is notoriously high. This problem has also troubled researchers in connectionism and has been a very important reason why the research on neural networks has not truly developed. Because once your model becomes large and complex, although you feel it is powerful, you cannot find good parameters to realize this power. Until later, in 1976, Seppo Linnainmaa proposed a clever algorithm, gradient descent, and in 1986, David Rumelhart, Geoffrey Hinton, and Ronald Williams jointly proposed the backpropagation algorithm, which truly solved this problem.

[0:28:00] Let's start with the simplest case. Suppose the other five knobs, except K1, have already been fixed. I'm telling you they've already been set to their optimal positions. Now, you only need to consider adjusting the K1 knob. So, how should we do it? Ah, we can adjust it and observe the change in the loss function. At this point, you'll find that the loss function has changed from having six input variables to only one variable, K1. Ah, this is a one-to-one changing function. We can easily plot it. The resulting graph will look something like this. Our goal is to find its minimum point. However, don't be misled by this graph. We are drawing it for illustrative purposes. In reality, we don't know the entire graph. We only know the loss function at a specific K1 and how much it is. So, we can only get a series of discrete points. For each input point, we know the function value. And in the positions between these points, how the loss function changes is completely unknown to us. You'll find that optimizing a neural network is even more complex than finding the minimum of a loss function because you cannot see the entire landscape of the loss function. It's like being placed on a complex terrain with varying altitudes, where each parameter's value is like longitude and latitude, and the altitude is the size of the loss function. A dense fog surrounds you, and you can only see the terrain directly beneath your feet. How do you descend to a lower altitude?

[0:29:14] Let's use the K1 example again. Earlier, we said something that wasn't entirely accurate: the information we have is actually a bit more than just the size of the loss function. Specifically, we can also know whether, at a certain position, the loss function increases or decreases as K1 increases. In mathematical terms, we can obtain the slope of the tangent line of the loss function at this point, which is more professionally called the derivative. You actually use this method when adjusting bathwater temperature or a radio. That is, you can first turn the knob a little bit in a certain direction, Delta x, and see if it's better or worse. For example, here, we start at position X0, and the loss function is Y0. Then we increase it by Delta x to position X1. Let's see what the loss function becomes. So we find that the loss function has increased by Delta y, meaning it has gotten worse. At this point, you know the bathwater should be adjusted in the opposite direction. So, when our adjustment change Delta x is infinitely small, the ratio of the change in function value Delta y to the change in Delta x will approach a constant value, which is the slope of the tangent line of the loss function at this point. This is the derivative of the function at this point. When the change is very small, the change in function value y is proportional to the change in x, and this ratio is the derivative. So, we can summarize the spirit of gradient descent in one sentence: we reduce it a little bit each time. We check each time to see where we should go to reduce the loss function at our current position, and then we move a small distance in that direction. Then, we check the derivative again and move. We repeat this process continuously, thus gradually reducing the loss function until we finally stop at the bottom, and the parameters hardly change. At this point, we have successfully reduced the loss function to a very low level.

[0:30:00] Now, we understand how to adjust one knob. But this has a very unrealistic premise: that the other five knobs have already been adjusted to their optimal state and are fixed. In reality, you need to adjust many knobs simultaneously, and none of them are perfectly adjusted. What's the use of this method? Ah, it's useful. In fact, the method we just discussed can be easily extended to more general and complex situations. For example, suppose you need to adjust two knobs, K1 and K2, simultaneously. At this point, the loss function becomes a binary function with two real number inputs and one real number output. It can be represented as a 2D surface. Ah, this is what many people often hear about: the loss surface. What is the derivative of a binary function now? Since there are two knobs, the direction of adjustment becomes ambiguous. Should we only adjust K1, only K2, or both? This involves the concept of partial derivatives. We can fix K2 and only change K1. At this point, we get the partial derivative of the loss function with respect to K1. Conversely, fix K1 and only change K2. At this point, we get the partial derivative with respect to K2. This corresponds to fixing one of K2 or K1 and individually adjusting the other knob, and observing its impact on the loss function output. Geometrically, this means we take two cross-sections perpendicular to the coordinate axes, intersect them with the surface, and the intersection will cut out a curve. Then we find the derivative of this curve. By combining these two derivatives, we get what you often hear about but may not know what it is: the gradient. Simply put, the gradient is the direction of fastest change in the function's value at a given point, and it is also the steepest direction of the surface locally. It is a 2D version of differentiation. With it, we can repeat the previous process, moving a small distance in the direction of the locally fastest decrease in the loss function. We can completely use a similar method to adjust two knobs simultaneously. This method is the famous gradient descent.

[0:31:30] You see, if two knobs can be adjusted, then this method can be applied to any number of knobs. The complete loss function is a complex six-dimensional surface. We can still follow the same procedure: for each knob, we fix the other knobs and look at the relationship between that knob and the loss function, and how it changes. Does increasing it increase or decrease the loss function? This way, we can obtain the partial derivative for each knob. By combining them, we get a six-dimensional gradient. Next, we only need each knob to iteratively decrease the loss function in its corresponding direction, thereby approximating the underlying patterns of these data.

[0:32:00] Now we know that gradient descent can optimize the network and find parameters that result in a relatively low loss function. However, when faced with a complex, stacked neural network, how do we calculate this gradient? This question is very technical. The answer is backpropagation, an algorithm specifically designed to calculate the gradients of complex neural networks. This is also the first step where many people learning deep learning get discouraged. Here, we will not go into the detailed specifics of backpropagation but will tell you its most essential idea. Whether it's a neural network or the polynomial fitting we discussed earlier, essentially, we are continuously combining and iterating simple basic operations like addition, subtraction, multiplication, division, squaring, exponentiation, etc., to form a huge, complex function. They are like building a massive machine from individual building blocks. What we care about is nothing more than the gradient of each knob parameter, specifically, the relationship between how the loss function changes when each knob moves slightly. And this information can be passed from back to front, layer by layer. Why? Because we know the derivatives of each basic building block very well, and we also understand how gradients combine and change during the process of combining blocks.

[0:33:12] You see, in middle school, we learned the basic rules of differentiation. The derivative of a sum is the sum of the derivatives. The derivative of a product is... and there's also the most important property: the chain rule. If we first input 'x' into a function 'g', and then use the output 'g(x)' as input for function 'f', the entire process combined is still a function that takes 'x' as input and outputs a value. It is f(g(x)). For example, if one is sine and the other is log, its graph will look something like this. The question is, we know the forms of 'f' and 'g' and their respective derivatives. How do we find the derivative of this combined function with respect to 'x'?

[0:33:50] Now, suppose we change the input 'x' by a Delta. According to the definition of the derivative, we know that when the input 'g(x)' changes by Delta, its output will change at a rate of g'(x). So, you know that this intermediate output g(x) will increase by g'(x) multiplied by Delta. Then, further, for the latter function f(x), note that its derivative is f'(g(x)). So, when its input changes by this amount, its output will be further multiplied by f'(g(x)) on this intermediate change, which is this much. When we divide the entire right side by the change in x, Delta, we get the chain rule for differentiation. That is, first compute g(x), then compute f(x), and then differentiate them together. The result is g'(x) multiplied by f'(g(x)). This is the differentiation of composite functions. If we use the building block analogy from before, you can imagine three gears meshing together. The angles they rotate represent x, g(x), and f(g(x)) respectively. And the derivative g'(x) represents the speed ratio of the second gear g(x) relative to the first gear x. And f'(g(x)) is the speed ratio of the third gear relative to the second gear g(x). If we want to know how much the third gear changes when the first gear x changes slightly, we simply multiply these two gear ratios together. This is the chain rule.

[0:34:57] With the chain rule, we can break down the process step by step from back to front to obtain the derivative of each parameter. This is because any parameter, from itself to the loss function, must go through a series of function compositions. The final output is the model's prediction itself. We can directly calculate the loss function and its derivative. Then, we can use the chain rule we just discussed, layer by layer from back to front, multiplying the nested derivative functions together. When we return to the initial K1 position, we obtain the gradient of K1 with respect to the entire output loss function. This algorithm is called backpropagation.

[0:35:30] So, to summarize, the method to find the best parameter settings for these millions of knobs in a machine is to use the backpropagation algorithm to calculate the derivative of each parameter, and then use the gradient descent method to change these parameters slightly each time, continuously evolving and moving towards better parameters. Finally, the entire neural network will magically understand and master the patterns in the data, learn the underlying functions, and acquire the intelligence we desire.

[0:35:59] In the previous content, we explained in detail that intelligence is about building a black box, and the structure and origin of neural networks, a powerful and universal black box, as well as how to train a neural network. However, there is still a very important question: how does this neural network black box generalize? You see, we just collected some data and trained it. For the inputs it has seen in our collected data, for example, this digit image, it should output 6, which is what we want. It's not surprising that it can correctly identify it as 6. But for other images it has never seen, how can the trained network simultaneously recognize other digits? This is like giving someone many problems, and they can solve the problems in the practice book you give them. It's not surprising. But how do they learn the methods to solve these problems and perform well on new problems in an exam?

[0:36:44] This question is actually profound. It relates to a very important issue that makes machine learning possible: generalization. This term sounds high-end, but as we just said, its original meaning is to extend, to generalize, to apply knowledge flexibly. Let's go back to the least squares and curve fitting example. Consider this question: for an input 'x' in the middle region, we have no 'y' data, but you still feel it should be within this range. Why? Precisely because these scattered data points outline a trend. When we use a continuous, smooth function like this to accurately describe this trend, we can use this function to infer the reasonable output for an unseen input value within the data. This is actually the simplest form of generalization. We understand this association trend and extend it to unseen input values. By understanding the underlying patterns, we can provide reasonable predictions and outputs in unknown situations.

[0:37:30] Similarly, neural networks also have generalization ability, and it's a strong generalization ability. Although it hasn't seen identical images in the training data, it can discover the trends and subtle correlations between these input images and their labels during the training process. This correlation is like the gap in the middle of the curve fitting we discussed earlier; it looks like it should be connected this way, so you can predict the function value there. It's just that in our real data, this trend might be very abstract and not as intuitive as curve fitting. And this is precisely where neural networks are powerful. You only need to provide data, and the underlying abstract trends and patterns can be learned by the neural network itself. Many industries and disciplines face similar problems: in different situations, there's a feeling or pattern that is difficult to calculate or describe with simple, clear mathematics. For example, in Go, does this piece of formation look good? Can it survive? Experienced Go players can tell at a glance; the jargon is "bad flavor." But how to learn this feeling and flavor is very complex. Also, when speaking, in the context of a sentence, what should follow? This is also a complex and subtle sense of language. But how to learn it is hard to say. Not to mention analyzing the structure of an entire protein from an amino acid sequence, which is an extremely abstract and complex pattern.

[0:38:33] Previously, we needed highly specialized knowledge to imitate human intelligence, and even then, we didn't do it well. With deep learning, you can disregard the specifics. Just find a suitably architected neural network, collect data, train and fit it. Then, this neural network can grasp the subtle connections between inputs and outputs described in the data and generalize them to any potential scenario it hasn't seen, often performing better than humans. This formulaic solution is very versatile and has swept across various fields, triggering the AI revolution of recent years.

[0:39:00] But are neural networks and deep learning omnipotent? The answer is of course no. Every time a powerful method can achieve previously unimaginable tasks, humans tend to treat it as magic. Although deep learning does simulate the structure of the brain's neurons, it is still very different from true human intelligence. We've all seen these memes: "How to distinguish between a Shiba Inu and a loaf of bread?" They say they have many similarities, like being yellow, being long and narrow, etc., making them difficult to distinguish. This was originally a meme, but for any machine learning algorithm, this is a fundamental and fatal problem. Because, you see, this model has always understood image content by inputting various image features and the label you want. So, what it understands during training is that a yellow, long, narrow object is bread. Therefore, when you give it a picture of a Shiba Inu outside its training set, it will make incorrect judgments because the Shiba Inu matches various characteristics of bread. This is essentially the subtle difference between correlation between concepts and causal relationships. It hasn't grasped causality. And this problem can never be avoided in the methodology of collecting data and training models. This is why many applications using neural networks for fortune-telling or predicting crime rates are widely criticized. Because models mistakenly treat co-occurrence in the dataset as a necessary connection. For example, seeing a Black person and assuming they will commit a crime.

[0:40:20] Even worse, you might never know what mysterious connections this powerful neural network black box has grasped, because neural networks are too powerful and complex, making it almost impossible for us to understand how they operate internally to produce the reasonable predictions we desire. A typical example is adversarial samples. These are two images. Can you see any difference between them? But if you show the first image to a highly accurate, state-of-the-art neural network, it thinks it's a panda. But for the second image, it believes it's a turtle with 99% confidence. Look closely, and you'll find that the second image has some very tiny noise added compared to the first. And this noise is not random; it's specifically designed to deceive neural networks. This type of image is called an adversarial sample. The understanding and research of this are still ongoing, and we still haven't fully understood it. It is generally believed that it triggers certain magical switches at the underlying level of neural networks. These noises, which appear chaotic to the human eye, have strong turtle characteristics and correlations for neural networks.

[0:41:16] Seeing this, do you still think neural networks are omnipotent? Regarding AI, a pressing concern for many is: will it make me unemployed? In general, deep learning and neural networks provide a completely new approach to intelligence. As long as you collect data and train a good neural network using the gradient descent method, this super black box can understand the patterns in the data, thereby generalizing and providing intelligent predictions for any unseen input scenario. This is indeed a revolution. It has enabled AI to solve many delicate, complex, and data-intensive tasks that previously only humans could perform. Therefore, if a job has sufficient data, a fixed pattern, and a relatively mechanical and routine nature, it is easy to collect a large amount of data to train AI, which can largely replace manual labor in basic daily use. From this perspective, professions like secretarial work, illustration, photography, translation, finance, and even entry-level programmers will face significant impact in the future.

[0:42:10] However, as mentioned earlier, today's artificial intelligence is by no means omnipotent. On the contrary, it is often an "artificial idiot." Even large models today still perform inadequately on many complex problems. When facing entirely new problems beyond the scope of their training data, AI often struggles to make reasonable judgments. In fact, I believe AI development is more likely to change the nature of work rather than completely replace it. For example, secretarial work may become more efficient due to AI intervention, with humans managing at the supervision and decision-making level, rather than personally handling trivial matters. Similarly, in design and creative fields like illustration and advertising, although AI can generate preliminary works and basic materials, truly moving creativity and inspiration still require human involvement. Likewise, it can help researchers write articles, correct grammar, and even perform some simple formula derivations and code writing. However, real research ideas still need to be conceived by oneself. The content generated by AI often lacks the depth, ambiguity, and complexity of human emotion, which is also why it struggles to surpass humans in certain areas. More importantly, current AI still lacks the ability to interact in the real world. The development of AI in fields like autonomous driving and robotics is far less optimistic than the public imagines.

[0:43:20] Therefore, in summary, AI will indeed impact some professions, especially those that are highly repetitive and have fixed patterns. However, it will also bring new opportunities. The key to the future lies in actively adapting to these changes, enhancing one's own skills to better collaborate with AI, rather than being replaced by it. Human creativity, emotion, and wisdom remain things that AI cannot simulate or completely surpass. You can also join the AI trend, starting with this video as a stepping stone. Watching it a few more times, if you can't beat it, join it – it's not a bad strategy.

[0:43:50] That's all for this video. Production was truly not easy, so I hope you will like and save it, and offer more support. If you want to see more in-depth popular science videos like this in the future, remember to follow me, Manshi, to guide you through the sea of learning without hardship. See you next time.

[0:30:00] Pitts and Mcculloch, in the journal Biophysical Review, jointly published "A Logical Calculus of the Ideas Immanent in Nervous Activity," proposing this mathematical model of neurons. Notice the title: "A Logical Calculus of the Ideas Immanent in Nervous Activity." This further indicates that connectionism and symbolism are actually of the same origin.

[0:30:25] In 1957, Frank Rosenblatt of Cornell University created the first perceptron with practical applications. This perceptron took image pixels as input, adjusted parameters through algorithms, and continuously learned. Ultimately, it could determine if an image contained a man or a woman, or a left arrow or a right arrow. This caused a sensation at the time. While it might not seem like much today, let me explain it from a different angle so you understand how remarkable it was.

[0:30:58] Computers and humans are very different. Problems that are difficult for humans, like multiplying two ten-digit numbers, are simple for computers. However, things that are simple for humans are very difficult for computers, such as understanding the content of an image. To a computer, an image is essentially a collection of pixel values. Therefore, recognizing the content of an image, in a mathematical sense, means being given a large chunk of numbers forming a matrix and being asked, "What is actually drawn here?" or, "Is this a man or a woman?" Human faces vary greatly; subtle changes in various facial features, lighting, and angles all result in different images, leading to drastic changes in the specific pixel values. We need to use pure computation to determine if the person in a real photo is male or female. Do you still think this entire mathematical problem, that is, discerning content from an image, is easy? Aren't human vision and the brain incredible? This field is called computer vision.

[0:31:56] Because of this, after inventing the perceptron, a 30-year-old Rosenblatt was full of ambition and eagerly held a press conference to discuss the bright future of his research, attracting significant media attention. For instance, a reporter from the renowned New York Times was full of praise for the perceptron's advanced capabilities. The report stated it was the nascent form of a walking, seeing, writing, self-replicating, and self-aware electronic computer. He called it an "electronic brain." The term "computer" itself also originated around this time. The article was very optimistic, estimating that with another $100,000, these concepts could be realized within a year. At that time, the perceptron would be able to recognize people and call them by name, and even translate and record spoken content into another language in real-time. However, as we know from reality, these things have only truly been achieved in recent years.

[0:32:50] So, whether it was genuine enthusiasm or a way to impress investors, humans tend to be romantic about things they don't understand and overly optimistic about the future. Historically, whenever artificial intelligence achieved even a small step forward, humans would begin to imagine infinite capabilities for it, envisioning scenarios of AI wars. It has always been this way.

[0:33:12] Returning to the initial question: How exactly do we build an intelligent black box that can learn and evolve? The perceptron is a simple example. Professionally speaking, these things are called models. The model itself determines the functional form of its output. For example, here, it's about taking a weighted combination of all inputs using Wi, subtracting a threshold b, and then activating it – that's the entire model's output. This has a functional form. However, it doesn't fully determine the entire function; there are still a series of values you need to adjust and set. For instance, each Wi and b here are called parameters. We only need to ensure the model is powerful enough when designing it; any function you want it to achieve can essentially be realized by setting these parameters. Then, we just need to let this model continuously adjust its parameters, continuously changing towards being more useful and conforming to our desired output pattern, and it can ultimately achieve powerful intelligence. This is the belief of connectionism.

[0:34:15] This approach was ambitious from the start and achieved remarkable results with the perceptron. However, connectionism eventually fell into a cold spell, even being dismissed as fraudulent by the entire world. From the very beginning, many scholars opposed connectionism. They felt it was merely mechanically simulating biological structures, and the neuron model was too simplistic. They also believed connectionism expected magic to happen through a chaotic connection of elements.

[0:34:45] In 1969, Marvin Minsky wrote a book titled "Perceptrons." Just as a thumbs-up gesture might not always be praise but could be a prelude to an attack, his book was not intended to promote perceptrons but to deliver a death blow, nailing shut their coffin. Minsky pointed out a fact in his book: there is a very basic operation in logic circuits called XOR. Simply put, when the two input features are the same, the output is 0; when they are different, the output is 1. It's such a simple correspondence, yet the perceptron could not accomplish it.

[0:35:22] Why is this the case? If we carefully examine the functional form of a perceptron, we find that it essentially calculates the sign of W1x + W2y - b. If we plot all such inputs of x and y on a 2D coordinate system, all inputs that allow the perceptron to activate satisfy W1x + W2y - b > 0. Those familiar with middle school math will recognize this as linear programming. Therefore, the dividing line between inputs that satisfy the condition for activation and those that do not will always be a straight line. However, for the XOR problem, you'll find that the two points that need to be activated and the two points that do not are distributed like this: you can never use a single straight line to perfectly separate these two types of points. Therefore, no single perceptron can directly implement this simple XOR operation.

[0:36:09] In his book, Minsky directly and unceremoniously stated that Rosenblatt's paper had no scientific value. Minsky himself received the Turing Award in the same year. This book consequently plunged the entire field of connectionism into obscurity. For the next two to three decades, the name "neural network" seemed to be synonymous with fraud, considered a useless plaything that couldn't even perform the XOR operation. With the leading figures like Turing Award winners denouncing it, neural networks entered an extreme cold spell. Everyone considered them garbage and scams. Funding significantly decreased, researchers switched fields, and AI research entered a decades-long winter.

[0:36:54] You see, humans are so prone to pessimism; a small counterexample can lead to abandoning a research direction full of potential. However, during this time, a group of researchers persisted. They eventually saw the dawn after the long night and became the founders of later deep learning, receiving Turing Awards. Let's listen to a clip from an interview with Geoffrey Hinton, a double recipient of both the Turing Award and Nobel Prize, when he was at the University of Toronto.

[0:37:22] "Looking back at your career, what aspects of it are you most proud of? I'm not necessarily just thinking about your discoveries, but maybe other aspects of your career as well, the people who worked with you, the teams that you've built. I guess I'm proud of the fact that I stuck with neural networks, even when people said they were rubbish, which was for about the first 40 years. But the intellectual achievement I'm most proud of is Boltzmann machines, which were an alternative to back propagation."

[0:37:55] How did they save neural networks? Returning to the XOR example, they thought, "If one neuron isn't enough, can multiple neurons do it?" For instance, let's take the outputs of these perceptrons one by one and then nest another layer of perceptrons behind them, serving as the input for the next perceptron. We create a layered, nested structure, layer by layer, forming a new perceptron. This way, we can have two neurons in the middle layer that are activated specifically by (0,1) and (1,0). For example, let the first neuron have combination coefficients of 1 and -1. In this case, only the input (1,0) will provide the strongest stimulus, with a value of 1; all others will not exceed 0. So, if we set a threshold of 1/2, it can be activated only when the input is (1,0). Similarly, we can set up another neuron to be activated only in the case of (0,1). The coefficient settings would be -1 and 1. This way, the two middle neurons can respectively focus on the two positions we want to activate. Then, we simply add the outputs of these two neurons. If the sum is greater than 0, we get the final activation. Thus, the entire model will activate precisely at (1,0) and (0,1), but not at (0,0) and (1,1), thereby achieving the XOR function.

[0:38:58] This is the later famous MLP, Multilayer Perceptron. Of course, the middle layer here can have more than two neurons, and the number of layers can also be more than two. When these neurons are stacked layer upon layer, it forms the renowned neural network. Each connection here signifies the connection strength between two neurons, which is an adjustable parameter coefficient. Computer scientists have proven that as long as the depth and width of this neural network are sufficiently large, it can theoretically approximate any function and express the input-output correspondence required for any intelligent behavior. In other words, as long as you have a super-large neural network, any intelligent black box function you desire can definitely be achieved by setting a set of parameters.

[0:39:46] How can we understand the powerful capability of neural networks? Do you recall what we said earlier? By combining different attribute concepts and then activating, we obtained the intelligence of a perceptron that could recognize apples. If we continue to nest perceptrons, we can continuously combine simple, basic concepts into more complex ones. For example, in this neural network for digit recognition, the neurons at the very front are responsible for recognizing very basic strokes and edges. Neurons in subsequent layers are then responsible for combining these basic features to recognize more complex concepts, such as circles, horizontal lines, vertical lines, broken lines, and so on. Then, deeper neurons can combine these lines and shapes to recognize complex digits. For instance, a '9' is a ring plus a tail at the bottom right. As the layers deepen, the neural network gradually deduces complex overall shapes from simple features, ultimately accurately recognizing a complex concept. And this entire process requires no intervention from human expert knowledge; it is completed automatically. This is precisely the power of neural networks.

[0:40:51] As time progresses, neural network technology continues to advance. The Multilayer Perceptron we saw earlier is just the most classic and basic type. How to design better and more powerful model structures has always been an important topic in deep learning. For instance, in the visual neural systems of real-world animals, neurons do not need to be densely connected to all neurons in the previous layer; they only need to connect to a few local neurons. Furthermore, the parameters and structure of each neuron's connection to the previous layer are similar. Therefore, when designing neural networks, we can draw inspiration from this to reduce parameters and computational load, thereby improving the performance of neural networks. This is the famous Convolutional Neural Network (CNN). Later, researchers discovered that stacking too many convolutional layers made training difficult, so they introduced a type of skip connection. This is the Residual Network (ResNet). Alternatively, you can create skip connections between any two layers; this is DenseNet. And then, the foundational framework of GPT today, the Transformer, which is attention, essentially represents a basic framework structure for a type of network, with a large number of parameters to be determined. A good structure allows the black box to learn faster and require less data. This is why neural network architecture design was once a very important field in deep learning.

[0:41:57] You will surely ask: Neural networks are so powerful, able to autonomously discover structures hidden in data and understand concepts. How exactly do they achieve this? The answer is through training with data, guiding the neural network to form intelligence through rewards and punishments. But how exactly should we reward and punish a neural network? In fact, from GPT to AlphaFold, to Midjourney and various reinforcement learning models, all complex and advanced artificial intelligence models, without exception, use the same algorithm to train the network and find the best parameters. This algorithm is called gradient descent.

[0:42:34] Special note: The mathematical knowledge in this part is extensive and highly technical, but because it is so crucial, we must cover it. Therefore, if you truly don't understand it, it's okay. Specifically, I will follow the video and ideas of YouTube vlogger Artem Kirsanov to explain. Before explaining gradient descent, let's briefly review what we've covered. We first mentioned that the essence of intelligence is a black box. This black box can find the correspondence between input and output from data. In other words, from the perspective of data-driven machine learning and statistical learning, intelligence is essentially about giving you a set of points and finding a function that describes their relationship. Here, x and y can be any two quantities you care about. As long as we learn a function that can characterize the trend of these points, we can obtain a reasonable output for any given input. In other words, intelligence is achieved.

[0:43:22] How do we find the underlying patterns outlined by these data points? Based on the previous content, you might think of neural networks. This is certainly one way. However, here, to understand gradient descent, let's use a simpler method to find this function. For example, let's linearly combine simple monomial modules like constants, x, x-squared, x-cubed, x-to-the-fourth, and x-to-the-fifth. In other words, we want to find a fifth-degree polynomial to characterize the pattern of these data changes. We need to find the best combination of the six parameters from K0 to K5. So, what constitutes a good parameter combination? We need a quantitative way to measure how well a set of coefficients for a polynomial fits the data. This is where the loss function comes in.

[0:44:05] In fact, you've all seen loss functions before. In middle school, you learned about the method of least squares. This essentially uses a simple linear function y = kx + b to build a black box. For each data point, there will be a deviation between the linear function's prediction and the actual result. We sum the squares of these deviations to get the loss function for this line. In complex nonlinear cases, the loss function is similar. We also sum the squared errors between the function's predicted value and the actual data point's value to get the loss function for this function. You can see that as the function's prediction more closely fits the trend of these data points, the total loss function will be smaller. Conversely, the loss function will be larger. Generally speaking, the loss function measures the discrepancy between a model's prediction and the true result. Just remember that understanding the pattern is equivalent to having a small loss function.

[0:44:59] Please note that two types of functions appear here. Do not confuse them. The first is the curve we use to fit the data points, which we call the fitting function, that is, the fifth-degree polynomial. Its input is x, and its output is y. We need to determine these six parameters. There are infinitely many possible functions from input to output. We want to find the best one. So, what is the best? For this, we introduce the loss function. It measures how good a fitting function is; it's a scoring mechanism. Its input is the six coefficients of the polynomial. After receiving these coefficients, it first constructs the fitted curve function, then compares and calculates the deviations at all data points, and sums their squares to get the final output of the loss function. We only need to find the input parameter combination K0 to K5 that minimizes this loss function. Then we can find an excellent fitting function. With this fitting function, we can take this fitted function machine, input any x we care about, and get a reasonable y that conforms to the data's pattern.

[0:45:57] You can think of it as playing a game. Each parameter k is a knob. By setting this coefficient, it produces a different polynomial curve. Your goal is to adjust these knobs to make this fitted curve closely match the data points. In fact, what neural networks do is fundamentally the same. Just replace k here with the connection coefficients and threshold b between neurons. Then, training a neural network is also a game of adjusting parameter knobs to reduce the loss function. The difficulty of this game lies in the fact that there are simply too many knobs. Look at this fifth-degree polynomial with six parameter knobs; it's already overwhelming. The number of parameters in a neural network is astronomically large. For example, GPT-3 has 175 billion parameters. In other words, you need to adjust over a hundred billion knobs simultaneously and ensure that the combination of these knob settings results in good performance, enabling it to converse and solve problems. Doesn't that sound incredible? This is almost an impossible task. Mathematically, this problem is called non-convex optimization, and its difficulty is notoriously high. This problem also plagued connectionist researchers and was a very important reason why this branch of neural network research did not truly develop. Because once your model becomes large and complex, even though you think it's powerful, you can't find good parameters to realize that power. It wasn't until later, in 1976, that Seppo Linnainmaa proposed a clever algorithm, gradient descent, and in 1986, David Rumelhart, Geoffrey Hinton, and Ronald Williams jointly proposed the backpropagation algorithm, that this problem was truly solved.

[0:47:34] Let's start with the simplest case. Assume that the five knobs other than K1 have already been fixed, and you've been told they've been set to their optimal positions. Now, you only need to consider adjusting the K1 knob. So, how should you do it? Well, we can adjust it and observe the change in the loss function. At this point, you'll find that the loss function, which was originally a function of six input variables, becomes a function of only one variable, K1. This is a one-to-one changing function, and we can easily plot it. The graph would look something like this. Our goal is to find its minimum point. However, don't be misled by this graph. We're drawing it for illustrative purposes. In reality, we don't know the entire graph. We only know what the fitted function looks like at a specific K1 value and calculate the corresponding loss function. Therefore, we can only obtain a series of discrete points. For each input point, we know the function value. But in the intermediate positions, we have no idea how the loss function changes. You'll find that optimizing a neural network is even more complex than finding the minimum of a loss function, because you cannot see the entire picture of the loss function. It's like being placed on a terrain with extremely complex ups and downs. The value of each parameter is like longitude and latitude, and the altitude is the magnitude of the loss function. It's foggy all around, and you can only see the terrain beneath your feet. How do you get down to a lower altitude?

[0:48:54] Let's use the K1 example again. Earlier, we said something that wasn't entirely accurate: the information we know is actually a bit more than just the magnitude of the loss function. Specifically, we can also know whether the loss function increases or decreases as K1 increases at a certain point. In mathematical terms, we can obtain the slope of the tangent line to the loss function at this point. More professionally, this is called the derivative. You've actually used this method when adjusting bathwater or a radio. That is, you can turn the knob a little bit in a certain direction, say by Delta x, and see if it's better or worse. For example, here, we start at position X0, and the loss function is Y0. After increasing by Delta x to position X1, we see that the loss function has become Y1 (correction: this should be a different Y value, let's call it Y1'). So, we find that the loss function has increased by Delta y, meaning it has worsened. At this point, you know that the bathwater should be adjusted in the opposite direction.

[0:49:48] So, when our adjustment change, Delta x, is infinitely small, the ratio of the change in y (Delta y) to the change in x (Delta x) will approach a constant value, which is the slope of the tangent line to the loss function at this point. This is precisely the derivative of the function at this point. When the change is very small, the change in the function value y is directly proportional to the change in x, and this ratio is the derivative. So, we can summarize the spirit of gradient descent in one sentence: we decrease it a little bit each time. We check each time to reduce the loss function, and then we decide which direction to go from our current position, and then we take a very small step in that direction. Then, we check the derivative again and move. By repeating this process, we can continuously reduce the loss function until we finally stop at the bottom, and the parameters barely change. At this point, we have successfully reduced the loss function to a very low level.

[0:50:40] Now, we understand how to adjust one knob, but this relies on a very unrealistic premise: that the other five knobs have already been set to their optimal states and are fixed. In reality, you need to adjust multiple knobs simultaneously, and all knobs are not properly adjusted. What's the use of this method? Well, it's useful. In fact, the method we just described can be easily extended to more general and complex situations. For example, suppose you need to adjust both K1 and K2 simultaneously. At this point, the loss function becomes a binary function where the input is two real numbers and the output is a real number. It can be represented as a 2D surface. This is what many people often refer to as the loss surface. Here, the loss surface for K1 and K2 looks like a bowl.

[0:51:24] Wait, what's the derivative of a binary function? Now there are two knobs, so the direction of adjustment becomes ambiguous. Should we only adjust K1, or only K2, or both? This is where the concept of partial derivatives comes in. We can fix K2 and only vary K1. At this point, we get the partial derivative of the loss function with respect to K1. Conversely, fixing K1 and varying only K2 gives us the partial derivative with respect to K2. This corresponds to the influence on the loss function's output when we fix one of K2 or K1 and individually adjust the other knob. Geometrically, this means we intersect the surface with two cross-sections perpendicular to the coordinate axes. The cross-section will cut out a curve, and then we find the derivative of this curve. By piecing these two derivatives together, we get what you often hear about but may not know what it is: the gradient.

[0:52:15] Simply put, the gradient is the direction in which the function value changes most rapidly at a given point, and it is the steepest direction of the surface locally. It's a 2D version of differentiation. With it, we can repeat the previous process: move a small distance in the direction of the locally fastest decrease in the loss function. Then, check the derivative again and move. We can thus simultaneously adjust two knobs happily. This method is the famous gradient descent. You might think, since two knobs can be done, this method can be applied to any number of knobs. The complete loss function is a complex 6D surface. We can still follow the same procedure: for each knob, we fix the other knobs and individually look at the relationship between that knob and the loss function. Does increasing it increase or decrease the loss function? This way, we get the partial derivative for each knob. Piecing them together, we get a 6D gradient. Next, we just need to let each knob iteratively decrease the loss function in its corresponding direction, thereby fitting the underlying patterns of this data.

[0:53:18] Now we know that gradient descent can optimize networks and find parameters that result in a relatively low loss function. However, when faced with a complex neural network with many stacked layers, how do we calculate this gradient? This is a very professional question, and the answer is backpropagation. This is a specialized algorithm for calculating gradients in complex neural networks, and it's the first step that deters many from learning deep learning. Here, we won't go into the detailed specifics of backpropagation but will tell you its most essential idea. Whether it's a neural network or the polynomial fitting we discussed earlier, essentially, we are using very simple basic operations like addition, subtraction, multiplication, division, squaring, exponentiation, etc., continuously combining, composing, and iterating to form a huge, complex function. They are like assembling a massive machine from individual building blocks. What we care about is the gradient of each knob parameter, that is, the relationship between how much the loss function changes when each knob is nudged a little bit.

[0:54:15] In the most straightforward terms, we care about the relationship between how much the loss function changes as each knob is nudged a little bit. This information can be passed from back to front, layer by layer, because we are very clear about the derivatives of each basic building block, and we also know how gradients combine and change during the composition of these blocks. You see, in middle school, we learned the basic rules of differentiation: the derivative of a sum is the sum of the derivatives, and the derivative of a product is... Besides addition and multiplication, there's another crucial property: the chain rule. If we first input x into a function g and then use the output g(x) as input for function f, the entire process combined is still a function that takes x as input and outputs a value; it's f(g(x)). For example, if one is sine and the other is log, its graph would look something like this. The question is, if we know the forms and derivatives of f and g individually, how do we find the derivative of this combined function with respect to x?

[0:55:16] Now, suppose we change the input x by Delta. According to the definition of the derivative, we know that when the input g(x) changes by Delta, its output will change proportionally to g'(x). So, you know that this intermediate output g(x) will increase by g'(x) multiplied by Delta. Then, further, for the latter function f(x), note that its derivative is f'(g(x)). So, when its input changes by this amount, its output will be multiplied by f'(g(x)) on this intermediate change, which is this much. When we divide this entire right side by the change in x, Delta, we get the chain rule for differentiation. That is, first compute g(x), then f(x), and the combined derivative is g'(x) multiplied by f'(g(x)). This is the differentiation of composite functions. If we use the building block analogy from before, imagine three gears interlocking. The angles they turn represent x, g(x), and f(g(x)) respectively. The derivative g'(x) represents the ratio of the speed of transmission of the second gear, g(x), relative to the first gear, x. And f'(g(x)) is the ratio of the speed of transmission of the third gear relative to the second gear, g(x). If we want to know how much the third gear changes when the first gear, x, changes slightly, we just need to multiply these two transmission ratios. This is the chain rule.

[0:56:30] With the chain rule, we can break it down step by step from back to front to obtain the derivative of each parameter. This is because any parameter, from its position to the loss function, must go through a series of function compositions, with the final layer's output being the model's prediction itself. We can directly calculate the loss function and its derivative. Then, using the chain rule we just discussed, we can multiply the derivative of the nested function layer by layer from back to front. When we return to the initial position of K1, we get the gradient of K1 with respect to the overall output, the loss function. This algorithm is called backpropagation.

[0:57:10] So, let's summarize. The method to find the best parameter settings for these millions of knobs is to use the backpropagation algorithm to calculate the derivative of each parameter, and then use the gradient descent method to change these parameters a little bit each time, continuously evolving and moving towards better parameters. Finally, the entire neural network will magically understand and grasp the patterns in the data, learn the underlying function, and acquire the desired intelligence.

[0:57:41] In the previous content, we explained in detail that intelligence is about building a black box, what the neural network, this powerful and universal black box, is, its construction and origin, and how to train a neural network. However, there is still a very important question: how does this neural network black box generalize? You see, we only collected some data and then trained it. For the input it has seen in our collected data, for example, this image of a digit '6', it should output '6'. It's not surprising that it can correctly recognize '6' for things it has seen. But for other images it has never seen, how can the trained network simultaneously recognize other digits? This is like giving someone many problems to solve; they can solve the problems in the practice book you give them, which is not surprising. But how do they learn the problem-solving methods so they can perform well on new exam questions?

[0:58:28] This question is profound and relates to a very important issue that makes machine learning viable: generalization. The term "generalization" sounds advanced, but as we just said, its meaning is essentially to extend, to apply knowledge to new situations, and to use it flexibly. Let's go back to the least squares method and the problem of curve fitting. For the input x in the middle position, we have no y data, but you still feel it should be within this range. Why? Exactly, because these scattered data points outline a trend. When we accurately describe this trend with a continuous, smooth function, we can use this function to infer what a reasonable output might be for a given input that we haven't seen in the data. This is a simple form of generalization. We understand this associated trend and extend it to unseen input values. By understanding the underlying pattern, we can provide reasonable predictions and outputs in unknown situations.

[0:59:19] Similarly, neural networks also have generalization capabilities, and they are strong generalization capabilities. Although they haven't seen identical images in the training data, they can discover the trends and subtle correlations between these input images and their labels during the training process. This correlation is like how, in curve fitting, the disconnected part in the middle looks like it should be connected this way, so you can predict the function value there. It's just that in our real data, this trend might be very abstract and not as intuitive as curve fitting. And this is precisely where neural networks are powerful. You only need to provide data, and the underlying abstract trends and patterns can be learned by the neural network itself. Many industries and disciplines face similar problems: in different contexts, there's a feeling or a pattern that is difficult to describe with simple, clear mathematics. For example, in Go, does this arrangement of stones look good? Can it survive? Experienced Go players can tell at a glance. The jargon is "bad flavor." But how to learn this feeling and flavor is very complex. Similarly, in speaking, given the preceding context of a sentence, what kind of phrase should follow? This is also a complex and subtle sense of language, but it's hard to say how to learn it. Not to mention analyzing the entire protein structure from amino acid sequences, which involves extremely abstract and complex patterns. Previously, we needed highly specialized knowledge to imitate human intelligence, and even then, we didn't do it well.

[1:00:35] With deep learning, you can ignore the specifics. Just find a suitably structured neural network, collect data, train it, and this neural network will grasp the subtle connections between the input and output described in the data and generalize. It can often perform better than humans in applying this knowledge to any potential scenario it hasn't seen. This formulaic solution is very universal, hence its sweeping adoption across various fields, triggering the AI revolution of recent years.

[1:01:05] However, are neural networks and deep learning omnipotent? The answer is certainly no. Whenever a powerful method emerges that can accomplish previously unimaginable tasks, humans tend to treat it as magic. Although deep learning does mimic the structure of the human brain's neurons, it is still very different from true human intelligence. We've all seen memes asking how to distinguish a Shiba Inu from a loaf of bread. They might have many similarities, such as being yellow and long. This was originally a joke, but for any machine learning algorithm, this is a fundamental and fatal problem. Because, you see, this model has always understood image content through various image features and the labels you assign. So, during training, it understands that a yellow, long object is a bread. Therefore, when you give it a picture of a Shiba Inu that is outside its training set, it might make an incorrect judgment because the Shiba Inu fits many characteristics of bread. This is essentially the subtle difference between correlation and causation between concepts. It hasn't grasped causation. And this problem can never be avoided with the methodology of collecting data and training models. This is why many applications using neural networks for fortune-telling or predicting crime rates are widely criticized. The model mistakenly treats co-occurrence in the dataset as a necessary connection. For example, seeing a black person and assuming they will commit a crime.

[1:02:25] Even worse, you might never know what mysterious connections this powerful neural network black box has grasped because neural networks are too powerful and complex, making it almost impossible for us to understand how they operate internally to provide the reasonable predictions we desire. A typical example is adversarial samples. These are two images. Can you tell the difference between them? However, if an accurate, state-of-the-art neural network is shown the first image, it thinks it's a panda. But with the second image, it believes it's a turtle with 99% confidence. Look closely, and you'll find that the second image has some very tiny noise added compared to the first. And this noise isn't random; it's specifically designed to deceive neural networks. This type of image is called an adversarial sample. Research into it is ongoing, and we still don't fully understand it. It's generally believed that it triggers certain magical switches at the underlying level of neural networks. To the human eye, this seemingly random noise has strong characteristics and correlations of a turtle for the neural network. Seeing this, do you still think neural networks are all-powerful?

[1:03:22] Regarding AI, a common concern is whether it will make us unemployed. Overall, deep learning and neural networks offer a completely new approach to intelligence. You just need to collect data, train a good neural network using gradient descent, and this super black box will understand the patterns in the data, thereby generalizing and providing intelligent predictions for any unknown input scenario. This is indeed a revolution. It has enabled AI to solve many complex tasks that previously only humans could perform, tasks involving subtle details and vast amounts of data. Therefore, if a job involves sufficient data, fixed patterns, and a relatively mechanical and routine nature, it's easy to collect a large amount of data for training. AI can significantly replace human labor in basic daily tasks. From this perspective, professions like administrative assistants, illustrators, photographers, translators, accountants, and even junior programmers will face significant impact in the future.

[1:04:17] However, as mentioned earlier, current artificial intelligence is far from omnipotent. On the contrary, it's often more like artificial idiocy. Even large models today still perform unsatisfactorily on many complex problems. When AI encounters entirely new problems beyond the scope of its training data, it often struggles to make reasonable judgments. In fact, I believe AI development is more likely to change the nature of work rather than completely replace it. For instance, administrative work may become more efficient with the intervention of AI. Humans will then focus on supervision and decision-making rather than personally handling tedious tasks. Similarly, in design and creative fields like illustration and advertising, although AI can generate initial drafts and basic materials, truly touching creativity and inspiration still requires human involvement. Likewise, it can help researchers write articles, correct grammar, and even perform some simple formula derivations and code writing. However, genuine research ideas still need to be conceived by oneself. AI-generated content often lacks the depth, ambiguity, and complexity of human emotion, which is also a fundamental reason why it struggles to surpass humans in certain areas.

[1:05:23] More importantly, current AI still lacks the ability to interact in the real world. In fields like autonomous driving and robotics, the pace of AI development is far less optimistic than popularly imagined. So, in summary, AI will indeed impact some professions, especially those with high repetition and fixed patterns. However, it will also bring new opportunities. The key in the future lies in actively adapting to these changes and upgrading one's skills to better collaborate with AI, rather than being replaced by it. Human creativity, emotion, and wisdom remain things that AI cannot simulate or completely surpass. You can also join the AI trend. Take this video as a starting point and begin learning. Watch it a few more times. If you can't beat them, join them – that's also a good strategy.

[1:06:03] That concludes this video. The production was truly not easy, so please like, save, and support it. If you want to see more in-depth popular science explanations like this in the future, remember to follow me. Meditation Math, guiding you through the sea of learning without hardship. We'll see you next time.

[1:00:00] In 1976, Seppo Linnainmaa proposed a clever algorithm, gradient descent, and in 1986, David Rumelhart, Geoffrey Hinton, and Ronald Williams jointly proposed the backpropagation algorithm, which truly solved the problem. Let's start with the simplest case. Suppose the other five knobs, except for K1, have already been fixed. I'm telling you, someone has already set them to their best positions. Now, you only need to consider how to adjust the K1 knob. How should you do it? Well, we can adjust it and observe the change in the loss function. You'll find that the loss function changes from having six input variables to having only one variable, K1. This is a one-to-one changing function. It's easy to plot, and the graph will look something like this. Our goal is to find its lowest point. However, don't be misled by this graph. We're drawing it for explanation. In reality, we don't know the entire graph. We only know what the fitting function looks like for a specific K1 value and what the corresponding loss function is. Therefore, we can only get a series of discrete points. For each input point, we know the function value. But in the positions between these points, we are completely unaware of how the loss function changes. You'll find that optimizing a neural network is even more complex than finding the minimum of a loss function. This is because you cannot see the entire picture of the loss function. It's like being placed on a highly complex, undulating terrain. Each parameter's value is like longitude and latitude, and the altitude is the loss function's magnitude. It's foggy all around, and you can only see the terrain right under your feet. How do you get down the mountain to a lower altitude? Let's use the K1 example again. Earlier, we said something that wasn't entirely accurate: the information we know is actually a bit more than just the pure magnitude of the loss function. Specifically, we can also know whether, at a certain position, the loss function increases or decreases as K1 increases. In mathematical terms, we can obtain the slope of the tangent line of the loss function at that point. The more professional term is the derivative. You've actually used this method when adjusting shower water temperature or tuning a radio. That is, you can turn the knob a little bit in a certain direction, Delta x, and see if it's better or worse. For example, here, we start at position X0, and the loss function is Y0. Then we increase it by Delta x to position X1. Let's see what the loss function becomes. So we find that the loss function has increased by Delta y, meaning it has gotten worse. At this point, you know that you should adjust the shower water in the opposite direction. So, when our adjustment change, Delta x, is infinitesimally small, the ratio of the change in y to the change in x will approach a constant value, which is the slope of the tangent line of the loss function at that point. This is the derivative of the function at that point. When the change is very small, the change in the function value y is directly proportional to the change in x, and this ratio is the derivative. So, we can summarize the spirit of gradient descent in one sentence: every time we decrease a little bit, we check where to go to reduce the loss function at our current position, and then we move a small distance in that direction. Then, we check the derivative again and move, constantly repeating the process. This way, we can continuously reduce the loss function until we finally stop at the bottom, and the parameters hardly change anymore. At this point, we have successfully reduced the loss function to a very low level. Now, we understand how to adjust one knob, but this is based on a very unrealistic premise: that the other five knobs have already been adjusted to their optimal states and are fixed. In reality, you have to adjust many knobs simultaneously, and none of them are perfectly adjusted. What's the use of this method? It is useful. In fact, this method can be easily extended to more general and complex situations. For example, suppose you need to adjust both K1 and K2 simultaneously. At this point, the loss function becomes a binary function with two real number inputs and one real number output. It can be represented as a 2D surface. This is what many people refer to as the loss surface. Here, the loss surface of K1 and K2 looks like a bowl. Wait, what is the derivative of a binary function? Now there are two knobs, so the direction of adjustment becomes ambiguous. Should we only adjust K1, or only K2, or both? This involves the concept of partial derivatives. We can fix K2 and only change K1. At this point, we get the partial derivative of the loss function with respect to K1. Conversely, we can fix K1 and only change K2. At this point, we get the partial derivative with respect to K2. This corresponds to the influence on the output of the loss function when we fix one of K1 or K2 and adjust the other knob separately. Geometrically, this means we intersect the surface with two planes perpendicular to the coordinate axes. The intersection will cut out a curve, and then we find the derivative of this curve. By putting these two derivatives together, we get the thing you often hear about but might not know what it is: the gradient. Simply put, the gradient is the direction of the fastest change in the function's value at a given point, and it is also the steepest direction of the surface locally. It's a 2D version of differentiation. With it, we can repeat the process described earlier: move a small distance in the direction of the steepest decrease in the loss function. We can completely use a similar method to adjust two knobs simultaneously. This method is the famous gradient descent. You might think, since two knobs can be adjusted this way, this method can be applied to any number of knobs. The complete loss function is a complex six-dimensional surface. We can still follow the same procedure. For each knob, we fix the other knobs and independently observe the relationship between that knob and the loss function. Does increasing it increase or decrease the loss function? This way, we can obtain the partial derivative of each knob. Putting them together, we get a 6-dimensional gradient. Next, we just need to make each knob iterate in its corresponding direction to continuously decrease the loss function, thereby fitting the underlying patterns of this data. Now we know that the gradient descent method can optimize networks and find parameters that result in a low loss function. However, when faced with a complex, multi-layered neural network, how do we calculate this gradient? This is a very professional question. The answer is backpropagation, an algorithm specifically designed to calculate the gradients of complex neural networks. It's also the first step that causes many people to give up on learning deep learning. Here, we won't go into the specific details of backpropagation, but we'll tell you its core idea. Whether it's a neural network or the polynomial fitting we discussed earlier, essentially, we use very simple basic operations like addition, subtraction, multiplication, division, squaring, exponentiation, etc., and continuously combine, compose, and iterate them to form a huge, complex function. These are like building a massive machine with individual building blocks. What we care about is the gradient of each knob parameter. In plain terms, we care about how the loss function changes when each knob moves a little bit. This information can be transmitted layer by layer from back to front. Why? Because we know the derivative of each basic building block very well, and we also know how gradients combine and change during the composition of these blocks. You know, we learned in middle school the basic rules of differentiation: the derivative of a sum is the sum of the derivatives, and the derivative of a product is... Besides the addition and multiplication mentioned above, there's another most important property: the chain rule. If we first input x into a function g, and then take the output g(x) as input to function f, then the entire process combined is still a function that takes x as input and outputs a numerical value. It's f(g(x)). For example, here, if one is sine and the other is log, its graph looks something like this. The problem is, we know the forms of f and g and their respective derivatives. How should we find the derivative of this composite function with respect to x? Now, suppose we change the input x by a Delta. According to the definition of the derivative, we know that when the input g(x) changes by Delta, its output will change proportionally by g'(x). So, you know that the intermediate output g(x) will increase by g'(x) multiplied by Delta. Then, further, for the function f(x) at the back, note that its derivative is f'(g(x)). So, when its input changes by this amount, its output will be further multiplied by f'(g(x)) on this intermediate change. When we divide the entire right side by the change in x, Delta, we get the chain rule of differentiation. That is, first, we compute g(x), then f(x), and the derivative of the combined function is g'(x) multiplied by f'(g(x)). This is the differentiation of composite functions. If we use the analogy of building blocks from before, you can imagine three gears meshing with each other. The angles they turn represent x, g(x), and f(g(x)) respectively. The derivative g'(x) represents the ratio of the speed of transmission of the second gear, g(x), to the first gear, x. And f'(g(x)) is the ratio of the speed of transmission of the third gear to the second gear, g(x). If we want to know how much the third gear changes when the first gear, x, changes a little bit, we just need to multiply these two transmission ratios. This is the chain rule. With the chain rule, we can decompose step by step from back to front to obtain the derivative of each parameter. This is because any parameter, from itself to the loss function, must go through a series of function compositions, and the final layer's output is the model's prediction itself. We can directly calculate the loss function and its derivative. Then, using the chain rule, we can layer by layer, from back to front, multiply the derivative functions of each nested layer step by step. When we return to the initial K1, we get the gradient of K1 with respect to the entire output loss function. This algorithm is called backpropagation. So, let's summarize: the method to find the best parameter settings for these millions of knobs is to use the backpropagation algorithm to calculate the derivative of each parameter, and then use the gradient descent method to change these parameters slightly each time, continuously evolving and moving towards better parameters. Finally, the entire neural network will magically understand and master the patterns in the data, learn the underlying functions, and acquire the intelligence we desire. In the previous content, we explained in detail that intelligence is about building a black box, the structure and origin of neural networks, this powerful, general black box, and how to train a neural network. However, there's still a very important question: how does this neural network black box generalize? You see, we just collect some data and train it. For the inputs seen in our collected data, for example, this digit image, it should output the desired output, 6. It's not surprising that it can correctly identify 6 because it has seen these things. But how can a trained network simultaneously recognize other digits for completely unseen images? This is like giving someone many problems to solve. It's not surprising that they can solve the problems in the practice book you give them. But how do they learn the problem-solving methods and can also do well on new problems in an exam? This question is actually profound. It relates to a very important issue that makes machine learning possible: generalization. The term "generalization" sounds advanced, but as we just said, its original meaning is to promote, apply knowledge flexibly, and use it effectively. Let's go back to the least squares method and curve fitting. Consider this question: for an input x in the middle position, we have no y data, but you still feel it should be within this range. Why? Yes, because these scattered data points outline a trend. When we accurately capture this trend with such a continuous and smooth function, we can use this function to infer the reasonable output for an input value that we haven't seen in the data. This is the simplest form of generalization. We understand this associated trend and extend it to input values we haven't seen, providing reasonable predictions and outputs in unknown situations by understanding the underlying patterns. Similarly, neural networks also have generalization capabilities, and they are strong. Although they haven't seen identical images in the training data, they can discover the trends and subtle correlations between these input images and labels during training. This correlation is like the intermediate broken part in curve fitting, where we predict the function value based on how it should connect. The only difference is that in our real data, this trend might be very abstract, not as intuitive as curve fitting. This is precisely where neural networks are powerful. You only need to provide data, and the underlying abstract trends and patterns will be learned by the neural network itself. Many industries and disciplines face similar problems: in different situations, there's a feeling or pattern that is difficult to describe with simple, clear mathematics. For example, in Go, whether a particular board position looks good or can survive is something an experienced player can tell at a glance, colloquially known as having "bad taste." But learning this feeling and taste is very complex. In speaking, what kind of sentence should follow in a given context is also a complex and subtle sense of language, but it's hard to say how to learn it. Not to mention analyzing the entire protein structure from amino acid sequences, which is an extremely abstract and complex pattern. Previously, we needed highly specialized knowledge to imitate human intelligence, and the imitation wasn't good. With deep learning, you can simply choose a suitable neural network architecture, collect data, train it, and this neural network will grasp the subtle connections between inputs and outputs described in the data and apply it to any potential input scenario it hasn't seen, often performing better than humans. This formulaic solution is very general, hence its widespread adoption across various fields and the AI revolution of recent years. But are neural networks and deep learning omnipotent? The answer is of course no. Whenever a powerful method can achieve previously unimaginable tasks, humans tend to treat it as magic. Although deep learning does mimic the structure of the human brain's neurons, there is still a significant difference between it and true human intelligence. We've all seen memes about how to distinguish between a Shiba Inu and a bread. They say they have many similarities, like being yellow and elongated, making them hard to distinguish. This was originally a meme, but for any machine learning algorithm, this is a fundamental and fatal problem. Because this model understands image content through various image features and the labels you assign. Therefore, during training, it understands that a yellow, elongated object is bread. Consequently, when you give it an image of a Shiba Inu that is outside its training set, it will make incorrect judgments because the Shiba Inu possesses various characteristics of bread. This is essentially the subtle distinction between correlation and causality between concepts, which it cannot grasp. And this problem can never be avoided with the methodology of collecting data and training models. This is why many applications using neural networks for fortune-telling or predicting crime rates are widely criticized. Because models mistakenly treat co-occurrence in the dataset as a necessary connection. For example, seeing a black person and assuming they will commit a crime. Even worse, you may never know what mysterious connections this powerful neural network black box has grasped. Because neural networks are too powerful and complex, we can hardly understand how they operate internally to provide the reasonable predictions we desire. A typical example is adversarial samples. Here are two images. Can you see any difference between them? But if you show the first image to a highly accurate, state-of-the-art neural network, it thinks it's a panda. But for the second image, it believes with 99% confidence that it's a turtle. If you look closely, you'll find that the second image has some very tiny noise added compared to the first. And this noise isn't random; it's specially designed to deceive neural networks. This type of image is called an adversarial sample. Its understanding and research are still ongoing, and we still haven't fully understood it. It is generally believed that it triggers some mysterious switches at the underlying level of neural networks. This noise, which appears chaotic to the human eye, has strong turtle characteristics and correlations to the neural network. Seeing this, do you still think neural networks are omnipotent? Many people are concerned about the impact of AI on their jobs. In general, deep learning and neural networks offer a completely new approach to intelligence. You just need to collect data and train a good neural network using gradient descent. This super black box can understand the patterns in the data, generalize, and provide intelligent predictions for any unknown input scenario. This is indeed a revolution. It has enabled AI to solve many subtle, complex, and data-intensive tasks that were previously only achievable by humans. Therefore, if a job has sufficient data and a fixed pattern, and is largely mechanical and routine, it's easy to collect a large amount of data for training. AI can largely replace human labor in basic daily tasks. From this perspective, professions like secretarial work, illustration, photography, translation, finance, and even entry-level programming will face significant impact in the future. However, as mentioned earlier, current AI is far from omnipotent. On the contrary, it's often artificial idiocy. Even current large models still perform inadequately on many complex problems. When facing completely new problems beyond the scope of the training data, AI often struggles to make reasonable judgments. In fact, I believe that AI development is more likely to change the nature of work rather than completely replace it. For example, secretarial work may become more efficient due to AI intervention, with humans managing at the supervision and decision-making levels, rather than personally handling trivial matters. Similarly, in design and creative fields like illustration and advertising, although AI can generate preliminary works and basic materials, truly touching creativity and inspiration still requires human involvement. Likewise, it can help researchers write articles, correct grammatical errors, and even perform simple formula derivations and code writing. However, the real research ideas still need to be conceived by oneself. AI-generated content often lacks the depth, ambiguity, and complexity of human emotion, which is why it's difficult to surpass humans in certain areas. More importantly, current AI still lacks the ability to interact with the real world. In fields like autonomous driving and robotics, the pace of AI development is far less optimistic than people imagine. Therefore, in conclusion, AI will indeed impact some professions, especially those that are highly repetitive and pattern-fixed. But at the same time, it will also bring new opportunities. The key to the future lies in actively adapting to these changes and enhancing one's skills to better collaborate with AI, rather than being replaced by it. Human creativity, emotion, and wisdom remain things that AI cannot simulate or completely surpass. You can also join the AI trend. Take this video as a starting point and begin learning. Watch it a few more times. If you can't beat them, join them. This is also a good strategy. That's all for this video. The production was truly not easy. I hope you all give it a like and save it for support. If you want to see more in-depth popular science content like this in the future, remember to follow me, Manshi's Deep Thoughts, Leading the Way in Learning is Not Hard. See you next time.